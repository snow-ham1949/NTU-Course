{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function\n",
    "import numpy as np\n",
    "import cv2\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "import torch\n",
    "from torch.utils.data import Dataset , DataLoader\n",
    "from torch.optim import Adam\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradReverse(Function):\n",
    "\tdef __init__(self , lambd):\n",
    "\t\tself.lambd = lambd\n",
    "\t\treturn\n",
    "\n",
    "\tdef forward(self , x):\n",
    "\t\treturn x.view_as(x)\n",
    "\n",
    "\tdef backward(self , grad_output):\n",
    "\t\treturn -self.lambd * grad_output\n",
    "\n",
    "def grad_reverse(x , lambd = 1.0):\n",
    "\treturn GradReverse(lambd)(x)\n",
    "\n",
    "class generator(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(generator , self).__init__()\n",
    "\t\tself.conv1 = nn.Conv2d(1 , 64 , kernel_size = (5 , 5) , stride = (1 , 1) , padding = (2 , 2))\n",
    "\t\tself.bn1 = nn.BatchNorm2d(64)\n",
    "\t\tself.conv2 = nn.Conv2d(64 , 64 , kernel_size = (5 , 5) , stride = (1 , 1) , padding = (2 , 2))\n",
    "\t\tself.bn2 = nn.BatchNorm2d(64)\n",
    "\t\tself.conv3 = nn.Conv2d(64 , 128 , kernel_size = (5 , 5) , stride = (1 , 1) , padding = (2 , 2))\n",
    "\t\tself.bn3 = nn.BatchNorm2d(128)\n",
    "\t\tself.fc1 = nn.Linear(8192 , 3072)\n",
    "\t\tself.bn4 = nn.BatchNorm1d(3072)\n",
    "\t\treturn\n",
    "\n",
    "\tdef forward(self , x):\n",
    "\t\tx = F.max_pool2d(F.relu(self.bn1(self.conv1(x))) , stride = (2 , 2) , kernel_size = (3 , 3) , padding = (1 , 1))\n",
    "\t\tx = F.max_pool2d(F.relu(self.bn2(self.conv2(x))) , stride = (2 , 2) , kernel_size = (3 , 3) , padding = (1 , 1))\n",
    "\t\tx = F.relu(self.bn3(self.conv3(x)))\n",
    "\t\tx = x.view(x.size(0) , 8192)\n",
    "\t\tx = F.relu(self.bn4(self.fc1(x)))\n",
    "\t\tx = F.dropout(x , training = self.training)\n",
    "\t\treturn x\n",
    "\n",
    "class classifier(nn.Module):\n",
    "\tdef __init__(self):\n",
    "\t\tsuper(classifier , self).__init__()\n",
    "\t\tself.fc1 = nn.Linear(3072 , 2048)\n",
    "\t\tself.bn1 = nn.BatchNorm1d(2048)\n",
    "\t\tself.fc2 = nn.Linear(2048 , 10)\n",
    "\t\treturn\n",
    "\n",
    "\tdef set_lambda(self , lambd):\n",
    "\t\tself.lambd = lambd\n",
    "\t\treturn\n",
    "\n",
    "\tdef forward(self , x , reverse = False):\n",
    "\t\tif (reverse):\n",
    "\t\t\tx = grad_reverse(x , self.lambd)\n",
    "\t\tx = F.relu(self.bn1(self.fc1(x)))\n",
    "\t\tx = self.fc2(x)\n",
    "\t\treturn x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class dataset(Dataset):\n",
    "\tdef __init__(self , data , label , return_label , transform , apply_transform):\n",
    "\t\tself.data = data\n",
    "\t\tself.label = label\n",
    "\t\tself.return_label = return_label\n",
    "\t\tself.transform = transform\n",
    "\t\tself.apply_transform = apply_transform\n",
    "\t\treturn\n",
    "\n",
    "\tdef __len__(self):\n",
    "\t\treturn self.data.shape[0]\n",
    "\n",
    "\tdef __getitem__(self , index):\n",
    "\t\tdata = self.data[index]\n",
    "\t\tif (self.apply_transform):\n",
    "\t\t\tdata = Image.fromarray(np.uint8(data))\n",
    "\t\t\tdata = self.transform(data)\n",
    "\t\t\tdata = np.asarray(data)\n",
    "\t\tdata = np.expand_dims(data , axis = 0)\n",
    "\t\tdata = data / 255\n",
    "\t\treturn (torch.FloatTensor(data) , self.label[index]) if (self.return_label) else torch.FloatTensor(data)\n",
    "\n",
    "def load_data():\n",
    "\ttemp_x = np.load(sys.argv[1])\n",
    "\ttrain_x = list()\n",
    "\tnumber_of_data = temp_x.shape[0]\n",
    "\tfor i in range(number_of_data):\n",
    "\t\timage = cv2.cvtColor(temp_x[i] , cv2.COLOR_BGR2GRAY)\n",
    "\t\timage = cv2.Canny(image , 250 , 300)\n",
    "\t\ttrain_x.append(image)\n",
    "\ttrain_x = np.array(train_x)\n",
    "\n",
    "\ttrain_y = np.load(sys.argv[2])\n",
    "\n",
    "\ttemp_x = np.load(sys.argv[3])\n",
    "\ttest_x = list()\n",
    "\tnumber_of_data = temp_x.shape[0]\n",
    "\tfor i in range(number_of_data):\n",
    "\t\timage = cv2.resize(temp_x[i] , (32 , 32) , cv2.INTER_LINEAR)\n",
    "\t\ttest_x.append(image)\n",
    "\ttest_x = np.array(test_x)\n",
    "\n",
    "\treturn (train_x , train_y , test_x)\n",
    "\n",
    "def discrepancy(output_1 , output_2):\n",
    "\treturn torch.mean(torch.abs(F.softmax(output_1 , dim = 1) - F.softmax(output_2 , dim = 1)))\n",
    "\n",
    "def train(train_x , train_y , test_x , generator , classifier_1 , classifier_2 , device):\n",
    "\t# Hyper-parameter.\n",
    "\tbatch_size = 128\n",
    "\tlearning_rate = 0.00002\n",
    "\tweight_decay = 0.0005\n",
    "\tepoch = 2000\n",
    "\n",
    "\ttransform = transforms.Compose([\n",
    "\t\ttransforms.RandomAffine(10 , translate = (0.1 , 0.1) , scale = (0.9 , 1.1)) ,\n",
    "\t\ttransforms.RandomHorizontalFlip()\n",
    "\t])\n",
    "\n",
    "\ttrain_dataset = dataset(train_x , train_y , True , transform , True)\n",
    "\ttest_dataset = dataset(test_x , None , False , transform , True)\n",
    "\ttrain_loader = DataLoader(train_dataset , batch_size = batch_size , shuffle = True)\n",
    "\ttest_loader = DataLoader(test_dataset , batch_size = batch_size , shuffle = True)\n",
    "\n",
    "\t(generator , classifier_1 , classifier_2) = (generator.to(device) , classifier_1.to(device) , classifier_2.to(device))\n",
    "\t(optimizer_generator , optimizer_classifier_1 , optimizer_classifier_2) = (Adam(generator.parameters() , lr = learning_rate , weight_decay = weight_decay) , Adam(classifier_1.parameters() , lr = learning_rate , weight_decay = weight_decay) , Adam(classifier_2.parameters() , lr = learning_rate , weight_decay = weight_decay))\n",
    "\tfor i in range(epoch):\n",
    "\t\tstart = time()\n",
    "\t\tgenerator.train()\n",
    "\t\tclassifier_1.train()\n",
    "\t\tclassifier_2.train()\n",
    "\t\tfor (j , ((data_source , label_source) , data_target)) in enumerate(zip(train_loader , test_loader)):\n",
    "\t\t\t(data_source , label_source , data_target) = (data_source.to(device) , label_source.to(device) , data_target.to(device))\n",
    "\t\t\t# Step 1\n",
    "\t\t\toptimizer_generator.zero_grad()\n",
    "\t\t\toptimizer_classifier_1.zero_grad()\n",
    "\t\t\toptimizer_classifier_2.zero_grad()\n",
    "\t\t\tfeature = generator(data_source)\n",
    "\t\t\ty_1 = classifier_1(feature)\n",
    "\t\t\ty_2 = classifier_2(feature)\n",
    "\t\t\tloss = F.cross_entropy(y_1 , label_source) + F.cross_entropy(y_2 , label_source)\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer_generator.step()\n",
    "\t\t\toptimizer_classifier_1.step()\n",
    "\t\t\toptimizer_classifier_2.step()\n",
    "\t\t\t# Step 2\n",
    "\t\t\toptimizer_generator.zero_grad()\n",
    "\t\t\toptimizer_classifier_1.zero_grad()\n",
    "\t\t\toptimizer_classifier_2.zero_grad()\n",
    "\t\t\tfeature = generator(data_source)\n",
    "\t\t\ty_1 = classifier_1(feature)\n",
    "\t\t\ty_2 = classifier_2(feature)\n",
    "\t\t\tloss_1 = F.cross_entropy(y_1 , label_source) + F.cross_entropy(y_2 , label_source)\n",
    "\t\t\tfeature = generator(data_target)\n",
    "\t\t\ty_1 = classifier_1(feature)\n",
    "\t\t\ty_2 = classifier_2(feature)\n",
    "\t\t\tloss_2 = discrepancy(y_1 , y_2)\n",
    "\t\t\tloss = loss_1 - loss_2\n",
    "\t\t\tloss.backward()\n",
    "\t\t\toptimizer_classifier_1.step()\n",
    "\t\t\toptimizer_classifier_2.step()\n",
    "\t\t\t# Step 3\n",
    "\t\t\tfor k in range(4):\n",
    "\t\t\t\tfeature = generator(data_target)\n",
    "\t\t\t\ty_1 = classifier_1(feature)\n",
    "\t\t\t\ty_2 = classifier_2(feature)\n",
    "\t\t\t\tloss = discrepancy(y_1 , y_2)\n",
    "\t\t\t\tloss.backward()\n",
    "\t\t\t\toptimizer_generator.step()\n",
    "\n",
    "\t\t\tif (j < min(len(train_loader) , len(test_loader)) - 1):\n",
    "\t\t\t\tm = int(50 * (j + 1) / min(len(train_loader) , len(test_loader)))\n",
    "\t\t\t\tbar = m * '=' + '>' + (49 - m) * ' '\n",
    "\t\t\t\tprint('epoch {}/{} [{}]'.format(i + 1 , epoch , bar) , end = '\\r')\n",
    "\t\t\telse:\n",
    "\t\t\t\tbar = 50 * '='\n",
    "\t\t\t\tend = time()\n",
    "\t\t\t\tprint('epoch {}/{} [{}] ({}s)'.format(i + 1 , epoch , bar , int(end - start)))\n",
    "\n",
    "\treturn (generator , classifier_1 , classifier_2)\n",
    "\n",
    "def main():\n",
    "\tos.environ['CUDA_VISIBLE_DEVICES'] = '0'\n",
    "\tdevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\t(train_x , train_y , test_x) = load_data()\n",
    "\t(generator , classifier_1 , classifier_2) = (MCD.generator() , MCD.classifier() , MCD.classifier())\n",
    "\t(generator , classifier_1 , classifier_2) = train(train_x , train_y , test_x , generator , classifier_1 , classifier_2 , device)\n",
    "\ttorch.save(generator.state_dict() , 'model/generator.pkl')\n",
    "\ttorch.save(classifier_1.state_dict() , 'model/classifier_1.pkl')\n",
    "\ttorch.save(classifier_2.state_dict() , 'model/classifier_2.pkl')\n",
    "\treturn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
