{"cells":[{"cell_type":"code","execution_count":50,"metadata":{"executionInfo":{"elapsed":6154,"status":"ok","timestamp":1655360297888,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"PUY8JBuNqTav"},"outputs":[],"source":["# Import some useful packages for this homework\n","import numpy as np\n","import pandas as pd\n","import torch\n","import os\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torchvision.transforms as transforms\n","from PIL import Image\n","from torch.utils.data import ConcatDataset, DataLoader, Subset, Dataset # \"ConcatDataset\" and \"Subset\" are possibly useful\n","from torchvision.datasets import DatasetFolder, VisionDataset\n","from torchsummary import summary\n","from tqdm.auto import tqdm\n","import random\n","\n","from sklearn.model_selection import KFold\n","import torchvision.models as models\n","import albumentations as A\n","from albumentations.pytorch import ToTensorV2\n","import cv2\n","from torch.autograd import Variable"]},{"cell_type":"code","execution_count":51,"metadata":{"executionInfo":{"elapsed":8,"status":"ok","timestamp":1655360297889,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"LLzp-naJqxv2"},"outputs":[],"source":["cfg = {\n","    'dataset_root': './food11-hw13',\n","    'save_dir': './outputs',\n","    'exp_name': \"boss_baseline\",\n","    'batch_size': 64,\n","    'lr': 0.001,\n","    'seed': 10006,\n","    'loss_fn_type': 'KD', # simple baseline: CE, medium baseline: KD. See the Knowledge_Distillation part for more information.\n","    'weight_decay': 0.0005,\n","    'grad_norm_max': 10,\n","    'n_epochs': 500, # train more steps to pass the medium baseline.\n","    'patience': 600,\n","}"]},{"cell_type":"code","execution_count":52,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":831,"status":"ok","timestamp":1655360298715,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"UEOn9dbcqzhH","outputId":"71e355a7-bf16-4d6a-9b75-e68aba6b05c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'dataset_root': './food11-hw13', 'save_dir': './outputs', 'exp_name': 'boss_baseline', 'batch_size': 64, 'lr': 0.001, 'seed': 10006, 'loss_fn_type': 'KD', 'weight_decay': 0.0005, 'grad_norm_max': 10, 'n_epochs': 500, 'patience': 600}\n"]}],"source":["myseed = cfg['seed']  # set a random seed for reproducibility\n","torch.backends.cudnn.deterministic = True\n","torch.backends.cudnn.benchmark = False\n","\n","def same_seed(myseed):\n","    np.random.seed(myseed)\n","    torch.manual_seed(myseed)\n","    random.seed(myseed)\n","    if torch.cuda.is_available():\n","        torch.cuda.manual_seed_all(myseed)\n","\n","same_seed(myseed)\n","\n","save_path = os.path.join(cfg['save_dir'], cfg['exp_name']) # create saving directory\n","os.makedirs(save_path, exist_ok=True)\n","\n","# define simple logging functionality\n","log_fw = open(f\"{save_path}/log.txt\", 'w') # open log file to save log outputs\n","def log(text):     # define a logging function to trace the training process\n","    print(text)\n","    log_fw.write(str(text)+'\\n')\n","    log_fw.flush()\n","\n","log(cfg)  # log your configs to the log file"]},{"cell_type":"code","execution_count":53,"metadata":{"executionInfo":{"elapsed":511,"status":"ok","timestamp":1655360299222,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"TKEcfdWqq1ix"},"outputs":[],"source":["normalize = A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n","# define training/testing transforms\n","test_tfm = A.Compose([\n","    A.Resize(256, 256),\n","    A.CenterCrop(224, 224),\n","    normalize,\n","    ToTensorV2(),\n","])\n","\n","train_tfm = A.Compose([\n","    A.Resize(256,256),\n","    A.CenterCrop(224,224),\n","    A.CLAHE(p=0.5),\n","    A.ShiftScaleRotate(shift_limit=0.0625, scale_limit=0.50, rotate_limit=45, p=0.5),\n","    A.Blur(blur_limit=3,p=0.5),\n","    A.RandomBrightnessContrast(p=0.5),\n","    normalize,\n","    ToTensorV2(),\n","])"]},{"cell_type":"code","execution_count":54,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1655360299222,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"DZidhvqBq6Y9"},"outputs":[],"source":["class FoodDataset(Dataset):\n","\n","    def __init__(self,path,train_tfm=train_tfm,test_tfm=test_tfm,files = None, mode='train'):\n","        super(FoodDataset).__init__()\n","        self.path = path\n","        self.files = sorted([os.path.join(path,x) for x in os.listdir(path) if x.endswith(\".jpg\")])\n","        if files != None:\n","            self.files = files\n","        print(f\"One {path} sample\",self.files[0])\n","        self.train_transform = train_tfm\n","        self.test_transform = test_tfm\n","        self.mode = mode\n","  \n","    def __len__(self):\n","        return len(self.files)\n","  \n","    def __getitem__(self,idx):\n","        fname = self.files[idx]\n","        # im = Image.open(fname)\n","        # im = self.transform(im)\n","        im = cv2.imread(fname)\n","        im = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\n","\n","        \n","        train_augmented = self.train_transform(image=im)\n","        train_im = train_augmented['image']\n","        #im = self.data[idx]\n","        try:\n","            label = int(fname.split(\"/\")[-1].split(\"_\")[0])\n","        except:\n","            label = -1 # test has no label\n","        \n","        if self.mode=='train':\n","            return train_im,label\n","        else:\n","            test_augmented = self.test_transform(image=im)\n","            test_im = test_augmented['image']\n","            train_ims = [train_im]\n","            for _ in range(4):\n","                temp = self.train_transform(image=im)['image']\n","                train_ims.append(temp)\n","            return train_ims, test_im, label\n","\n"]},{"cell_type":"code","execution_count":55,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":65399,"status":"ok","timestamp":1655360364617,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"JErdoWOerDlN","outputId":"92090a3e-527f-4c04-cdf7-83dd6a924876"},"outputs":[{"name":"stdout","output_type":"stream","text":["One ./food11-hw13/training sample ./food11-hw13/training/0_0.jpg\n","One ./food11-hw13/validation sample ./food11-hw13/validation/0_0.jpg\n"]}],"source":["# Form train/valid dataloaders\n","train_set = FoodDataset(os.path.join(cfg['dataset_root'],\"training\"), mode='train')\n","train_loader = DataLoader(train_set, batch_size=cfg['batch_size'], shuffle=True, num_workers=0, pin_memory=True)\n","\n","valid_set = FoodDataset(os.path.join(cfg['dataset_root'], \"validation\"), mode='test')\n","valid_loader = DataLoader(valid_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"]},{"cell_type":"code","execution_count":56,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1655360364617,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"FBANOdObrFXy"},"outputs":[],"source":["# Example implementation of Depthwise and Pointwise Convolution \n","def dwpw_conv(in_channels, out_channels, kernel_size, stride=1, padding=0):\n","    return nn.Sequential(\n","        nn.Conv2d(in_channels, in_channels, kernel_size, stride=stride, padding=padding, groups=in_channels), #depthwise convolution\n","        nn.Conv2d(in_channels, out_channels, 1), # pointwise convolution\n","    )"]},{"cell_type":"code","execution_count":57,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1655360364617,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"Tw6KZD7l56wc"},"outputs":[],"source":["#reference: https://github.com/Offliners/NTUML-2021Spring/blob/main/HW13/homework13.ipynb\n","\n","import math\n","\n","def conv_bn(inp, oup, stride):\n","    return nn.Sequential(\n","        nn.Conv2d(inp, oup, 3, stride, 1, bias=False),\n","        nn.BatchNorm2d(oup),\n","        nn.ReLU6(inplace=True)\n","    )\n","\n","\n","def conv_1x1_bn(inp, oup):\n","    return nn.Sequential(\n","        nn.Conv2d(inp, oup, 1, 1, 0, bias=False),\n","        nn.BatchNorm2d(oup),\n","        nn.ReLU6(inplace=True)\n","    )\n","\n","\n","def make_divisible(x, divisible_by=8):\n","    import numpy as np\n","    return int(np.ceil(x * 1. / divisible_by) * divisible_by)\n","\n","\n","class InvertedResidual(nn.Module):\n","    def __init__(self, inp, oup, stride, expand_ratio):\n","        super(InvertedResidual, self).__init__()\n","        self.stride = stride\n","        assert stride in [1, 2]\n","\n","        hidden_dim = int(inp * expand_ratio)\n","        self.use_res_connect = self.stride == 1 and inp == oup\n","\n","        if expand_ratio == 1:\n","            self.conv = nn.Sequential(\n","                # dw\n","                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                nn.ReLU6(inplace=True),\n","                # pw-linear\n","                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(oup),\n","            )\n","        else:\n","            self.conv = nn.Sequential(\n","                # pw\n","                nn.Conv2d(inp, hidden_dim, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                nn.ReLU6(inplace=True),\n","                # dw\n","                nn.Conv2d(hidden_dim, hidden_dim, 3, stride, 1, groups=hidden_dim, bias=False),\n","                nn.BatchNorm2d(hidden_dim),\n","                nn.ReLU6(inplace=True),\n","                # pw-linear\n","                nn.Conv2d(hidden_dim, oup, 1, 1, 0, bias=False),\n","                nn.BatchNorm2d(oup),\n","            )\n","\n","    def forward(self, x):\n","        if self.use_res_connect:\n","            return x + self.conv(x)\n","        else:\n","            return self.conv(x)\n","\n","\n","class MobileNetV2(nn.Module):\n","    def __init__(self, n_class=11, input_size=128, width_mult=1.):\n","        super(MobileNetV2, self).__init__()\n","        block = InvertedResidual\n","        input_channel = 32\n","        last_channel = 100\n","        interverted_residual_setting = [\n","            # t, c, n, s\n","            [1, 16, 1, 1],\n","            [6, 24, 1, 2],\n","            [6, 32, 1, 2],\n","            [6, 64, 2, 2],\n","        ]\n","\n","        # building first layer\n","        assert input_size % 32 == 0\n","        # input_channel = make_divisible(input_channel * width_mult)  # first channel is always 32!\n","        self.last_channel = make_divisible(last_channel * width_mult) if width_mult > 1.0 else last_channel\n","        self.features = [conv_bn(3, input_channel, 2)]\n","        # building inverted residual blocks\n","        for t, c, n, s in interverted_residual_setting:\n","            output_channel = make_divisible(c * width_mult) if t > 1 else c\n","            for i in range(n):\n","                if i == 0:\n","                    self.features.append(block(input_channel, output_channel, s, expand_ratio=t))\n","                else:\n","                    self.features.append(block(input_channel, output_channel, 1, expand_ratio=t))\n","                input_channel = output_channel\n","        # building last several layers\n","        self.features.append(conv_1x1_bn(input_channel, self.last_channel))\n","        # make it nn.Sequential\n","        self.features = nn.Sequential(*self.features)\n","\n","        # building classifier\n","        self.classifier = nn.Linear(self.last_channel, n_class)\n","\n","        self._initialize_weights()\n","\n","    def forward(self, x):\n","        x = self.features(x)\n","        x = x.mean(3).mean(2)\n","        x = self.classifier(x)\n","        return x\n","\n","    def _initialize_weights(self):\n","        for m in self.modules():\n","            if isinstance(m, nn.Conv2d):\n","                n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n","                m.weight.data.normal_(0, math.sqrt(2. / n))\n","                if m.bias is not None:\n","                    m.bias.data.zero_()\n","            elif isinstance(m, nn.BatchNorm2d):\n","                m.weight.data.fill_(1)\n","                m.bias.data.zero_()\n","            elif isinstance(m, nn.Linear):\n","                n = m.weight.size(1)\n","                m.weight.data.normal_(0, 0.01)\n","                m.bias.data.zero_()\n","\n","\n","def mobilenet_v2():\n","    model = MobileNetV2(width_mult=1)\n","\n","    return model\n","\n","def get_student_model(): \n","    return MobileNetV2()"]},{"cell_type":"code","execution_count":58,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":580,"status":"ok","timestamp":1655360365195,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"twmy9iUbrIrw","outputId":"a4d9f060-ac8e-4f60-e856-7f90c16832ca"},"outputs":[{"name":"stdout","output_type":"stream","text":["----------------------------------------------------------------\n","        Layer (type)               Output Shape         Param #\n","================================================================\n","            Conv2d-1         [-1, 32, 112, 112]             864\n","       BatchNorm2d-2         [-1, 32, 112, 112]              64\n","             ReLU6-3         [-1, 32, 112, 112]               0\n","            Conv2d-4         [-1, 32, 112, 112]             288\n","       BatchNorm2d-5         [-1, 32, 112, 112]              64\n","             ReLU6-6         [-1, 32, 112, 112]               0\n","            Conv2d-7         [-1, 16, 112, 112]             512\n","       BatchNorm2d-8         [-1, 16, 112, 112]              32\n","  InvertedResidual-9         [-1, 16, 112, 112]               0\n","           Conv2d-10         [-1, 96, 112, 112]           1,536\n","      BatchNorm2d-11         [-1, 96, 112, 112]             192\n","            ReLU6-12         [-1, 96, 112, 112]               0\n","           Conv2d-13           [-1, 96, 56, 56]             864\n","      BatchNorm2d-14           [-1, 96, 56, 56]             192\n","            ReLU6-15           [-1, 96, 56, 56]               0\n","           Conv2d-16           [-1, 24, 56, 56]           2,304\n","      BatchNorm2d-17           [-1, 24, 56, 56]              48\n"," InvertedResidual-18           [-1, 24, 56, 56]               0\n","           Conv2d-19          [-1, 144, 56, 56]           3,456\n","      BatchNorm2d-20          [-1, 144, 56, 56]             288\n","            ReLU6-21          [-1, 144, 56, 56]               0\n","           Conv2d-22          [-1, 144, 28, 28]           1,296\n","      BatchNorm2d-23          [-1, 144, 28, 28]             288\n","            ReLU6-24          [-1, 144, 28, 28]               0\n","           Conv2d-25           [-1, 32, 28, 28]           4,608\n","      BatchNorm2d-26           [-1, 32, 28, 28]              64\n"," InvertedResidual-27           [-1, 32, 28, 28]               0\n","           Conv2d-28          [-1, 192, 28, 28]           6,144\n","      BatchNorm2d-29          [-1, 192, 28, 28]             384\n","            ReLU6-30          [-1, 192, 28, 28]               0\n","           Conv2d-31          [-1, 192, 14, 14]           1,728\n","      BatchNorm2d-32          [-1, 192, 14, 14]             384\n","            ReLU6-33          [-1, 192, 14, 14]               0\n","           Conv2d-34           [-1, 64, 14, 14]          12,288\n","      BatchNorm2d-35           [-1, 64, 14, 14]             128\n"," InvertedResidual-36           [-1, 64, 14, 14]               0\n","           Conv2d-37          [-1, 384, 14, 14]          24,576\n","      BatchNorm2d-38          [-1, 384, 14, 14]             768\n","            ReLU6-39          [-1, 384, 14, 14]               0\n","           Conv2d-40          [-1, 384, 14, 14]           3,456\n","      BatchNorm2d-41          [-1, 384, 14, 14]             768\n","            ReLU6-42          [-1, 384, 14, 14]               0\n","           Conv2d-43           [-1, 64, 14, 14]          24,576\n","      BatchNorm2d-44           [-1, 64, 14, 14]             128\n"," InvertedResidual-45           [-1, 64, 14, 14]               0\n","           Conv2d-46          [-1, 100, 14, 14]           6,400\n","      BatchNorm2d-47          [-1, 100, 14, 14]             200\n","            ReLU6-48          [-1, 100, 14, 14]               0\n","           Linear-49                   [-1, 11]           1,111\n","================================================================\n","Total params: 99,999\n","Trainable params: 99,999\n","Non-trainable params: 0\n","----------------------------------------------------------------\n","Input size (MB): 0.57\n","Forward/backward pass size (MB): 81.41\n","Params size (MB): 0.38\n","Estimated Total Size (MB): 82.37\n","----------------------------------------------------------------\n"]}],"source":["# DO NOT modify this block and please make sure that this block can run sucessfully. \n","student_model = MobileNetV2()\n","# student_model = torch.hub.load('pytorch/vision:v0.10.0', 'shufflenet_v2_x1_0', pretrained=False)\n","summary(student_model, (3, 224, 224), device='cpu')\n","# You have to copy&paste the results of this block to HW13 GradeScope. "]},{"cell_type":"code","execution_count":59,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2835,"status":"ok","timestamp":1655360368027,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"wQ5WWCIjrKaM","outputId":"1ac3face-f921-4b73-8221-4a01055ddfc6"},"outputs":[{"name":"stderr","output_type":"stream","text":["Using cache found in /home/csypt/.cache/torch/hub/pytorch_vision_v0.10.0\n"]},{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":59,"metadata":{},"output_type":"execute_result"}],"source":["# Load provided teacher model (model architecture: resnet18, num_classes=11, test-acc ~= 89.9%)\n","teacher_model = torch.hub.load('pytorch/vision:v0.10.0', 'resnet18', pretrained=False, num_classes=11)\n","# load state dict\n","teacher_ckpt_path = os.path.join(cfg['dataset_root'], \"resnet18_teacher.ckpt\")\n","torch.save(teacher_model.state_dict(), teacher_ckpt_path)\n","teacher_model.load_state_dict(torch.load(teacher_ckpt_path, map_location='cpu'))\n","# Now you already know the teacher model's architecture. You can take advantage of it if you want to pass the strong or boss baseline. \n","# Source code of resnet in pytorch: (https://github.com/pytorch/vision/blob/main/torchvision/models/resnet.py)\n","# You can also see the summary of teacher model. There are 11,182,155 parameters totally in the teacher model\n","# summary(teacher_model, (3, 224, 224), device='cpu')"]},{"cell_type":"code","execution_count":60,"metadata":{"executionInfo":{"elapsed":5,"status":"ok","timestamp":1655360368027,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"MUIMxqO5rPN1"},"outputs":[],"source":["# Implement the loss function with KL divergence loss for knowledge distillation.\n","# You also have to copy-paste this whole block to HW13 GradeScope. \n","def loss_fn_kd(student_logits, labels, teacher_logits, alpha=0.5, T=20):\n","    # ------------TODO-------------\n","    # Refer to the above formula and finish the loss function for knowkedge distillation using KL divergence loss and CE loss.\n","    # If you have no idea, please take a look at the provided useful link above.\n","    original = (1. - alpha) * F.cross_entropy(student_logits, labels) \n","    loss = nn.KLDivLoss(reduction='batchmean')(F.log_softmax(student_logits/T, dim=1), F.softmax(teacher_logits/T, dim=1)) * alpha * T * T\n","    loss = loss + original\n","    return loss"]},{"cell_type":"code","execution_count":61,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12231,"status":"ok","timestamp":1655360380255,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"LXe4W69erSIO","outputId":"d4204756-13d3-4c95-96cd-1bdfdbd0ff11"},"outputs":[{"name":"stdout","output_type":"stream","text":["device: cuda\n"]},{"data":{"text/plain":["ResNet(\n","  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n","  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","  (relu): ReLU(inplace=True)\n","  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n","  (layer1): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer2): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer3): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (layer4): Sequential(\n","    (0): BasicBlock(\n","      (conv1): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (downsample): Sequential(\n","        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n","        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (1): BasicBlock(\n","      (conv1): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (relu): ReLU(inplace=True)\n","      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n","      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","    )\n","  )\n","  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n","  (fc): Linear(in_features=512, out_features=11, bias=True)\n",")"]},"execution_count":61,"metadata":{},"output_type":"execute_result"}],"source":["# choose the loss function by the config\n","if cfg['loss_fn_type'] == 'CE':\n","    # For the classification task, we use cross-entropy as the default loss function.\n","    loss_fn = nn.CrossEntropyLoss() # loss function for simple baseline.\n","\n","if cfg['loss_fn_type'] == 'KD': # KD stands for knowledge distillation\n","    loss_fn = loss_fn_kd # implement loss_fn_kd for the report question and the medium baseline.\n","\n","# You can also adopt other types of knowledge distillation techniques for strong and boss baseline, but use function name other than `loss_fn_kd`\n","# For example:\n","# def loss_fn_custom_kd():\n","#     pass\n","# if cfg['loss_fn_type'] == 'custom_kd':\n","#     loss_fn = loss_fn_custom_kd\n","\n","# \"cuda\" only when GPUs are available.\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","log(f\"device: {device}\")\n","\n","# The number of training epochs and patience.\n","n_epochs = cfg['n_epochs']\n","patience = cfg['patience'] # If no improvement in 'patience' epochs, early stop\n","\n","teacher_model.to(device)"]},{"cell_type":"code","execution_count":62,"metadata":{"executionInfo":{"elapsed":13,"status":"ok","timestamp":1655360380256,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"YHd3OCxR1HCh"},"outputs":[],"source":["#ref: https://github.com/facebookresearch/mixup-cifar10/blob/main/train.py\n","def mixup_data(x, y, alpha = 0.5):\n","    '''Returns mixed inputs, pairs of targets, and lambda'''\n","    if alpha > 0:\n","        lam = np.random.beta(alpha, alpha)\n","    else:\n","        lam = 1\n","\n","    batch_size = x.size()[0]\n","    index = torch.randperm(batch_size).cuda()\n","    mixed_x = lam * x + (1 - lam) * x[index, :]\n","    y_a, y_b = y, y[index]\n","    return mixed_x, y_a, y_b, lam\n","\n","def mixup_criterion(criterion, pred, teacher_pred, y_a, y_b, lam):\n","    return lam * criterion(pred, y_a, teacher_pred) + (1 - lam) * criterion(pred, y_b, teacher_pred)"]},{"cell_type":"code","execution_count":63,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1655360380256,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"JO4sdQ451LCF"},"outputs":[],"source":["def reset_weights(m):\n","  '''\n","    Try resetting model weights to avoid\n","    weight leakage.\n","  '''\n","  for layer in m.children():\n","   if hasattr(layer, 'reset_parameters'):\n","    layer.reset_parameters()"]},{"cell_type":"code","execution_count":64,"metadata":{},"outputs":[{"data":{"text/plain":["<All keys matched successfully>"]},"execution_count":64,"metadata":{},"output_type":"execute_result"}],"source":["ckpt_path = f\"{save_path}/student_best.ckpt\"\n","student_model.load_state_dict(torch.load(ckpt_path, map_location='cpu'))"]},{"cell_type":"code","execution_count":65,"metadata":{"id":"oUpkbGyFrUiH"},"outputs":[{"name":"stdout","output_type":"stream","text":["[ Train | 001/500 ] loss = 0.77856, acc = 0.76718\n","[ Valid | 001/500 ] loss = 0.83830, acc = 0.67843 -> best\n","Best model found at epoch 0, saving model\n","[ Train | 002/500 ] loss = 0.78286, acc = 0.76465\n","[ Valid | 002/500 ] loss = 0.85776, acc = 0.73353 -> best\n","Best model found at epoch 1, saving model\n","[ Train | 003/500 ] loss = 0.77414, acc = 0.77154\n","[ Valid | 003/500 ] loss = 0.82752, acc = 0.70496\n","[ Train | 004/500 ] loss = 0.77907, acc = 0.76789\n","[ Valid | 004/500 ] loss = 0.85847, acc = 0.71429\n","[ Train | 005/500 ] loss = 0.78263, acc = 0.76404\n","[ Valid | 005/500 ] loss = 0.86135, acc = 0.68513\n","[ Train | 006/500 ] loss = 0.77999, acc = 0.76982\n","[ Valid | 006/500 ] loss = 0.83180, acc = 0.73382 -> best\n","Best model found at epoch 5, saving model\n","[ Train | 007/500 ] loss = 0.77891, acc = 0.76738\n","[ Valid | 007/500 ] loss = 0.82016, acc = 0.72595\n","[ Train | 008/500 ] loss = 0.77871, acc = 0.76394\n","[ Valid | 008/500 ] loss = 0.84177, acc = 0.67259\n","[ Train | 009/500 ] loss = 0.78081, acc = 0.76191\n","[ Valid | 009/500 ] loss = 0.82518, acc = 0.73353\n","[ Train | 010/500 ] loss = 0.78124, acc = 0.76799\n","[ Valid | 010/500 ] loss = 0.83971, acc = 0.75131 -> best\n","Best model found at epoch 9, saving model\n","[ Train | 011/500 ] loss = 0.78372, acc = 0.75998\n","[ Valid | 011/500 ] loss = 0.84337, acc = 0.70496\n","[ Train | 012/500 ] loss = 0.77923, acc = 0.76485\n","[ Valid | 012/500 ] loss = 0.83897, acc = 0.72012\n","[ Train | 013/500 ] loss = 0.78072, acc = 0.76688\n","[ Valid | 013/500 ] loss = 0.84023, acc = 0.70087\n","[ Train | 014/500 ] loss = 0.77570, acc = 0.77012\n","[ Valid | 014/500 ] loss = 0.82154, acc = 0.72536\n","[ Train | 015/500 ] loss = 0.78188, acc = 0.76556\n","[ Valid | 015/500 ] loss = 0.83436, acc = 0.71399\n","[ Train | 016/500 ] loss = 0.78295, acc = 0.75968\n","[ Valid | 016/500 ] loss = 0.83146, acc = 0.72216\n","[ Train | 017/500 ] loss = 0.77120, acc = 0.77316\n","[ Valid | 017/500 ] loss = 0.83585, acc = 0.70087\n","[ Train | 018/500 ] loss = 0.77290, acc = 0.77154\n","[ Valid | 018/500 ] loss = 0.87267, acc = 0.71050\n","[ Train | 019/500 ] loss = 0.77759, acc = 0.76627\n","[ Valid | 019/500 ] loss = 0.83252, acc = 0.72536\n","[ Train | 020/500 ] loss = 0.77802, acc = 0.76830\n","[ Valid | 020/500 ] loss = 0.87189, acc = 0.70117\n","[ Train | 021/500 ] loss = 0.77533, acc = 0.77164\n","[ Valid | 021/500 ] loss = 0.84499, acc = 0.70058\n","[ Train | 022/500 ] loss = 0.77593, acc = 0.77053\n","[ Valid | 022/500 ] loss = 0.79955, acc = 0.72945\n","[ Train | 023/500 ] loss = 0.77530, acc = 0.76728\n","[ Valid | 023/500 ] loss = 0.81578, acc = 0.76676 -> best\n","Best model found at epoch 22, saving model\n","[ Train | 024/500 ] loss = 0.77212, acc = 0.77093\n","[ Valid | 024/500 ] loss = 0.85056, acc = 0.71166\n","[ Train | 025/500 ] loss = 0.78226, acc = 0.76150\n","[ Valid | 025/500 ] loss = 0.86051, acc = 0.69271\n","[ Train | 026/500 ] loss = 0.77133, acc = 0.77346\n","[ Valid | 026/500 ] loss = 0.81843, acc = 0.75743\n","[ Train | 027/500 ] loss = 0.77624, acc = 0.76627\n","[ Valid | 027/500 ] loss = 0.84826, acc = 0.70612\n","[ Train | 028/500 ] loss = 0.77481, acc = 0.76748\n","[ Valid | 028/500 ] loss = 0.81870, acc = 0.71429\n","[ Train | 029/500 ] loss = 0.77418, acc = 0.76718\n","[ Valid | 029/500 ] loss = 0.81075, acc = 0.73236\n","[ Train | 030/500 ] loss = 0.77608, acc = 0.76667\n","[ Valid | 030/500 ] loss = 0.81427, acc = 0.73120\n","[ Train | 031/500 ] loss = 0.77486, acc = 0.76819\n","[ Valid | 031/500 ] loss = 0.84230, acc = 0.70583\n","[ Train | 032/500 ] loss = 0.77340, acc = 0.76992\n","[ Valid | 032/500 ] loss = 0.81914, acc = 0.72245\n","[ Train | 033/500 ] loss = 0.77884, acc = 0.76313\n","[ Valid | 033/500 ] loss = 0.80591, acc = 0.74781\n","[ Train | 034/500 ] loss = 0.77510, acc = 0.76789\n","[ Valid | 034/500 ] loss = 0.81025, acc = 0.74985\n","[ Train | 035/500 ] loss = 0.77154, acc = 0.76890\n","[ Valid | 035/500 ] loss = 0.83570, acc = 0.73469\n","[ Train | 036/500 ] loss = 0.77474, acc = 0.76809\n","[ Valid | 036/500 ] loss = 0.81788, acc = 0.73499\n","[ Train | 037/500 ] loss = 0.77273, acc = 0.77387\n","[ Valid | 037/500 ] loss = 0.81711, acc = 0.75131\n","[ Train | 038/500 ] loss = 0.77419, acc = 0.77022\n","[ Valid | 038/500 ] loss = 0.82788, acc = 0.70962\n","[ Train | 039/500 ] loss = 0.77441, acc = 0.77073\n","[ Valid | 039/500 ] loss = 0.82859, acc = 0.74082\n","[ Train | 040/500 ] loss = 0.77385, acc = 0.77529\n","[ Valid | 040/500 ] loss = 0.81662, acc = 0.73324\n","[ Train | 041/500 ] loss = 0.77503, acc = 0.76961\n","[ Valid | 041/500 ] loss = 0.83038, acc = 0.72799\n","[ Train | 042/500 ] loss = 0.77247, acc = 0.76688\n","[ Valid | 042/500 ] loss = 0.83953, acc = 0.72886\n","[ Train | 043/500 ] loss = 0.77213, acc = 0.77316\n","[ Valid | 043/500 ] loss = 0.82289, acc = 0.71166\n","[ Train | 044/500 ] loss = 0.77443, acc = 0.76647\n","[ Valid | 044/500 ] loss = 0.84617, acc = 0.72245\n","[ Train | 045/500 ] loss = 0.77226, acc = 0.77519\n","[ Valid | 045/500 ] loss = 0.80874, acc = 0.73790\n","[ Train | 046/500 ] loss = 0.77204, acc = 0.76596\n","[ Valid | 046/500 ] loss = 0.85837, acc = 0.69854\n","[ Train | 047/500 ] loss = 0.77340, acc = 0.77032\n","[ Valid | 047/500 ] loss = 0.82966, acc = 0.72974\n","[ Train | 048/500 ] loss = 0.77170, acc = 0.77164\n","[ Valid | 048/500 ] loss = 0.81445, acc = 0.72741\n","[ Train | 049/500 ] loss = 0.77398, acc = 0.76586\n","[ Valid | 049/500 ] loss = 0.80140, acc = 0.73994\n","[ Train | 050/500 ] loss = 0.76890, acc = 0.77640\n","[ Valid | 050/500 ] loss = 0.81068, acc = 0.73673\n","[ Train | 051/500 ] loss = 0.74416, acc = 0.79749\n","[ Valid | 051/500 ] loss = 0.77884, acc = 0.77843 -> best\n","Best model found at epoch 50, saving model\n","[ Train | 052/500 ] loss = 0.73557, acc = 0.80580\n","[ Valid | 052/500 ] loss = 0.76824, acc = 0.76997\n","[ Train | 053/500 ] loss = 0.73052, acc = 0.81887\n","[ Valid | 053/500 ] loss = 0.76942, acc = 0.77522\n","[ Train | 054/500 ] loss = 0.73363, acc = 0.81259\n","[ Valid | 054/500 ] loss = 0.77522, acc = 0.76589\n","[ Train | 055/500 ] loss = 0.73483, acc = 0.81462\n","[ Valid | 055/500 ] loss = 0.78518, acc = 0.75743\n","[ Train | 056/500 ] loss = 0.73098, acc = 0.81451\n","[ Valid | 056/500 ] loss = 0.77999, acc = 0.77522\n","[ Train | 057/500 ] loss = 0.73221, acc = 0.81472\n","[ Valid | 057/500 ] loss = 0.78132, acc = 0.77201\n","[ Train | 058/500 ] loss = 0.72902, acc = 0.82141\n","[ Valid | 058/500 ] loss = 0.78045, acc = 0.76647\n","[ Train | 059/500 ] loss = 0.72850, acc = 0.81603\n","[ Valid | 059/500 ] loss = 0.78574, acc = 0.77347\n","[ Train | 060/500 ] loss = 0.72826, acc = 0.82222\n","[ Valid | 060/500 ] loss = 0.77787, acc = 0.76297\n","[ Train | 061/500 ] loss = 0.72992, acc = 0.81299\n","[ Valid | 061/500 ] loss = 0.78517, acc = 0.76297\n","[ Train | 062/500 ] loss = 0.72593, acc = 0.82475\n","[ Valid | 062/500 ] loss = 0.78612, acc = 0.78047 -> best\n","Best model found at epoch 61, saving model\n","[ Train | 063/500 ] loss = 0.72465, acc = 0.82374\n","[ Valid | 063/500 ] loss = 0.79947, acc = 0.74169\n","[ Train | 064/500 ] loss = 0.72420, acc = 0.82424\n","[ Valid | 064/500 ] loss = 0.77980, acc = 0.77988\n","[ Train | 065/500 ] loss = 0.72812, acc = 0.81948\n","[ Valid | 065/500 ] loss = 0.78267, acc = 0.76472\n","[ Train | 066/500 ] loss = 0.72409, acc = 0.82445\n","[ Valid | 066/500 ] loss = 0.77688, acc = 0.77493\n","[ Train | 067/500 ] loss = 0.72250, acc = 0.82536\n","[ Valid | 067/500 ] loss = 0.77486, acc = 0.78426 -> best\n","Best model found at epoch 66, saving model\n","[ Train | 068/500 ] loss = 0.72470, acc = 0.82272\n","[ Valid | 068/500 ] loss = 0.78092, acc = 0.74665\n","[ Train | 069/500 ] loss = 0.72284, acc = 0.82739\n","[ Valid | 069/500 ] loss = 0.77942, acc = 0.77551\n","[ Train | 070/500 ] loss = 0.72244, acc = 0.82546\n","[ Valid | 070/500 ] loss = 0.79207, acc = 0.74373\n","[ Train | 071/500 ] loss = 0.72699, acc = 0.81837\n","[ Valid | 071/500 ] loss = 0.77414, acc = 0.79038 -> best\n","Best model found at epoch 70, saving model\n","[ Train | 072/500 ] loss = 0.72261, acc = 0.82830\n","[ Valid | 072/500 ] loss = 0.78457, acc = 0.75015\n","[ Train | 073/500 ] loss = 0.72271, acc = 0.82800\n","[ Valid | 073/500 ] loss = 0.77410, acc = 0.76472\n","[ Train | 074/500 ] loss = 0.72180, acc = 0.82637\n","[ Valid | 074/500 ] loss = 0.78452, acc = 0.76618\n","[ Train | 075/500 ] loss = 0.72575, acc = 0.82333\n","[ Valid | 075/500 ] loss = 0.78566, acc = 0.79417 -> best\n","Best model found at epoch 74, saving model\n","[ Train | 076/500 ] loss = 0.72758, acc = 0.82171\n","[ Valid | 076/500 ] loss = 0.78597, acc = 0.75714\n","[ Train | 077/500 ] loss = 0.72662, acc = 0.82364\n","[ Valid | 077/500 ] loss = 0.78461, acc = 0.78251\n","[ Train | 078/500 ] loss = 0.72400, acc = 0.82566\n","[ Valid | 078/500 ] loss = 0.78442, acc = 0.77638\n","[ Train | 079/500 ] loss = 0.72289, acc = 0.83033\n","[ Valid | 079/500 ] loss = 0.77948, acc = 0.75569\n","[ Train | 080/500 ] loss = 0.72811, acc = 0.82009\n","[ Valid | 080/500 ] loss = 0.78243, acc = 0.75569\n","[ Train | 081/500 ] loss = 0.72276, acc = 0.82789\n","[ Valid | 081/500 ] loss = 0.78154, acc = 0.76531\n","[ Train | 082/500 ] loss = 0.72041, acc = 0.82668\n","[ Valid | 082/500 ] loss = 0.78401, acc = 0.77318\n","[ Train | 083/500 ] loss = 0.72048, acc = 0.82962\n","[ Valid | 083/500 ] loss = 0.77955, acc = 0.75160\n","[ Train | 084/500 ] loss = 0.72247, acc = 0.82587\n","[ Valid | 084/500 ] loss = 0.78878, acc = 0.76676\n","[ Train | 085/500 ] loss = 0.71777, acc = 0.83327\n","[ Valid | 085/500 ] loss = 0.78705, acc = 0.76851\n","[ Train | 086/500 ] loss = 0.72006, acc = 0.82840\n","[ Valid | 086/500 ] loss = 0.78835, acc = 0.75335\n","[ Train | 087/500 ] loss = 0.72222, acc = 0.82506\n","[ Valid | 087/500 ] loss = 0.79883, acc = 0.76472\n","[ Train | 088/500 ] loss = 0.71855, acc = 0.83124\n","[ Valid | 088/500 ] loss = 0.78213, acc = 0.76210\n","[ Train | 089/500 ] loss = 0.72006, acc = 0.83144\n","[ Valid | 089/500 ] loss = 0.77643, acc = 0.77638\n","[ Train | 090/500 ] loss = 0.72086, acc = 0.82333\n","[ Valid | 090/500 ] loss = 0.79093, acc = 0.77055\n","[ Train | 091/500 ] loss = 0.72350, acc = 0.82080\n","[ Valid | 091/500 ] loss = 0.79163, acc = 0.75860\n","[ Train | 092/500 ] loss = 0.71685, acc = 0.83144\n","[ Valid | 092/500 ] loss = 0.77570, acc = 0.77930\n","[ Train | 093/500 ] loss = 0.72156, acc = 0.82749\n","[ Valid | 093/500 ] loss = 0.77577, acc = 0.75743\n","[ Train | 094/500 ] loss = 0.72001, acc = 0.82972\n","[ Valid | 094/500 ] loss = 0.77859, acc = 0.77726\n","[ Train | 095/500 ] loss = 0.72185, acc = 0.82891\n","[ Valid | 095/500 ] loss = 0.78433, acc = 0.76006\n","[ Train | 096/500 ] loss = 0.71834, acc = 0.83104\n","[ Valid | 096/500 ] loss = 0.78482, acc = 0.77493\n","[ Train | 097/500 ] loss = 0.71819, acc = 0.82992\n","[ Valid | 097/500 ] loss = 0.77687, acc = 0.80262 -> best\n","Best model found at epoch 96, saving model\n","[ Train | 098/500 ] loss = 0.72328, acc = 0.82718\n","[ Valid | 098/500 ] loss = 0.77824, acc = 0.78921\n","[ Train | 099/500 ] loss = 0.72339, acc = 0.82293\n","[ Valid | 099/500 ] loss = 0.77225, acc = 0.77026\n","[ Train | 100/500 ] loss = 0.71854, acc = 0.82911\n","[ Valid | 100/500 ] loss = 0.78146, acc = 0.76152\n","[ Train | 101/500 ] loss = 0.70165, acc = 0.84867\n","[ Valid | 101/500 ] loss = 0.75472, acc = 0.79388\n","[ Train | 102/500 ] loss = 0.69941, acc = 0.84776\n","[ Valid | 102/500 ] loss = 0.75925, acc = 0.78601\n","[ Train | 103/500 ] loss = 0.69618, acc = 0.85283\n","[ Valid | 103/500 ] loss = 0.75592, acc = 0.80641 -> best\n","Best model found at epoch 102, saving model\n","[ Train | 104/500 ] loss = 0.69580, acc = 0.85161\n","[ Valid | 104/500 ] loss = 0.76389, acc = 0.78950\n","[ Train | 105/500 ] loss = 0.69612, acc = 0.85648\n","[ Valid | 105/500 ] loss = 0.75756, acc = 0.78484\n","[ Train | 106/500 ] loss = 0.69676, acc = 0.84665\n","[ Valid | 106/500 ] loss = 0.76524, acc = 0.76968\n","[ Train | 107/500 ] loss = 0.69139, acc = 0.86073\n","[ Valid | 107/500 ] loss = 0.75939, acc = 0.79592\n","[ Train | 108/500 ] loss = 0.69532, acc = 0.85029\n","[ Valid | 108/500 ] loss = 0.75859, acc = 0.77959\n","[ Train | 109/500 ] loss = 0.69478, acc = 0.85425\n","[ Valid | 109/500 ] loss = 0.75551, acc = 0.78892\n","[ Train | 110/500 ] loss = 0.69384, acc = 0.85222\n","[ Valid | 110/500 ] loss = 0.75241, acc = 0.78338\n","[ Train | 111/500 ] loss = 0.69174, acc = 0.85820\n","[ Valid | 111/500 ] loss = 0.76066, acc = 0.78105\n","[ Train | 112/500 ] loss = 0.69377, acc = 0.85029\n","[ Valid | 112/500 ] loss = 0.76328, acc = 0.78309\n","[ Train | 113/500 ] loss = 0.69224, acc = 0.85536\n","[ Valid | 113/500 ] loss = 0.75758, acc = 0.78280\n","[ Train | 114/500 ] loss = 0.69192, acc = 0.85719\n","[ Valid | 114/500 ] loss = 0.75694, acc = 0.78484\n","[ Train | 115/500 ] loss = 0.68977, acc = 0.85992\n","[ Valid | 115/500 ] loss = 0.75947, acc = 0.77959\n","[ Train | 116/500 ] loss = 0.69639, acc = 0.85161\n","[ Valid | 116/500 ] loss = 0.76513, acc = 0.77522\n","[ Train | 117/500 ] loss = 0.69244, acc = 0.85749\n","[ Valid | 117/500 ] loss = 0.75653, acc = 0.78863\n","[ Train | 118/500 ] loss = 0.69215, acc = 0.85597\n","[ Valid | 118/500 ] loss = 0.76023, acc = 0.79329\n","[ Train | 119/500 ] loss = 0.69396, acc = 0.85486\n","[ Valid | 119/500 ] loss = 0.75935, acc = 0.79009\n","[ Train | 120/500 ] loss = 0.69057, acc = 0.86266\n","[ Valid | 120/500 ] loss = 0.75826, acc = 0.78396\n","[ Train | 121/500 ] loss = 0.69147, acc = 0.85952\n","[ Valid | 121/500 ] loss = 0.76523, acc = 0.77901\n","[ Train | 122/500 ] loss = 0.68812, acc = 0.86327\n","[ Valid | 122/500 ] loss = 0.76191, acc = 0.78863\n","[ Train | 123/500 ] loss = 0.69159, acc = 0.85475\n","[ Valid | 123/500 ] loss = 0.76410, acc = 0.76822\n","[ Train | 124/500 ] loss = 0.69026, acc = 0.86013\n","[ Valid | 124/500 ] loss = 0.76983, acc = 0.77551\n","[ Train | 125/500 ] loss = 0.69141, acc = 0.86013\n","[ Valid | 125/500 ] loss = 0.76307, acc = 0.77143\n","[ Train | 126/500 ] loss = 0.68965, acc = 0.86590\n","[ Valid | 126/500 ] loss = 0.75942, acc = 0.79184\n","[ Train | 127/500 ] loss = 0.69126, acc = 0.85708\n","[ Valid | 127/500 ] loss = 0.76436, acc = 0.76385\n","[ Train | 128/500 ] loss = 0.69172, acc = 0.85911\n","[ Valid | 128/500 ] loss = 0.75848, acc = 0.77668\n","[ Train | 129/500 ] loss = 0.69084, acc = 0.85982\n","[ Valid | 129/500 ] loss = 0.75508, acc = 0.79913\n","[ Train | 130/500 ] loss = 0.68980, acc = 0.85972\n","[ Valid | 130/500 ] loss = 0.75798, acc = 0.78834\n","[ Train | 131/500 ] loss = 0.69128, acc = 0.85769\n","[ Valid | 131/500 ] loss = 0.75770, acc = 0.78601\n","[ Train | 132/500 ] loss = 0.69197, acc = 0.86124\n","[ Valid | 132/500 ] loss = 0.76194, acc = 0.77522\n","[ Train | 133/500 ] loss = 0.69179, acc = 0.85992\n","[ Valid | 133/500 ] loss = 0.76171, acc = 0.78396\n","[ Train | 134/500 ] loss = 0.69009, acc = 0.86489\n","[ Valid | 134/500 ] loss = 0.75689, acc = 0.78484\n","[ Train | 135/500 ] loss = 0.68848, acc = 0.86469\n","[ Valid | 135/500 ] loss = 0.75888, acc = 0.78222\n","[ Train | 136/500 ] loss = 0.69055, acc = 0.85658\n","[ Valid | 136/500 ] loss = 0.76435, acc = 0.77872\n","[ Train | 137/500 ] loss = 0.69396, acc = 0.85597\n","[ Valid | 137/500 ] loss = 0.75704, acc = 0.78746\n","[ Train | 138/500 ] loss = 0.68912, acc = 0.86357\n","[ Valid | 138/500 ] loss = 0.76113, acc = 0.78105\n","[ Train | 139/500 ] loss = 0.68754, acc = 0.86398\n","[ Valid | 139/500 ] loss = 0.75594, acc = 0.78950\n","[ Train | 140/500 ] loss = 0.68710, acc = 0.86469\n","[ Valid | 140/500 ] loss = 0.75974, acc = 0.77580\n","[ Train | 141/500 ] loss = 0.68881, acc = 0.86195\n","[ Valid | 141/500 ] loss = 0.76063, acc = 0.77493\n","[ Train | 142/500 ] loss = 0.68703, acc = 0.86550\n","[ Valid | 142/500 ] loss = 0.76467, acc = 0.77755\n","[ Train | 143/500 ] loss = 0.68913, acc = 0.86327\n","[ Valid | 143/500 ] loss = 0.77094, acc = 0.77901\n","[ Train | 144/500 ] loss = 0.68805, acc = 0.86337\n","[ Valid | 144/500 ] loss = 0.76540, acc = 0.77872\n","[ Train | 145/500 ] loss = 0.68995, acc = 0.86236\n","[ Valid | 145/500 ] loss = 0.75284, acc = 0.79534\n","[ Train | 146/500 ] loss = 0.68848, acc = 0.86236\n","[ Valid | 146/500 ] loss = 0.75815, acc = 0.78950\n","[ Train | 147/500 ] loss = 0.68653, acc = 0.86388\n","[ Valid | 147/500 ] loss = 0.75910, acc = 0.78688\n","[ Train | 148/500 ] loss = 0.69089, acc = 0.86104\n","[ Valid | 148/500 ] loss = 0.76345, acc = 0.77493\n","[ Train | 149/500 ] loss = 0.69275, acc = 0.85708\n","[ Valid | 149/500 ] loss = 0.75643, acc = 0.78950\n","[ Train | 150/500 ] loss = 0.68997, acc = 0.85739\n","[ Valid | 150/500 ] loss = 0.75681, acc = 0.78659\n","[ Train | 151/500 ] loss = 0.67759, acc = 0.87563\n","[ Valid | 151/500 ] loss = 0.75227, acc = 0.78192\n","[ Train | 152/500 ] loss = 0.68208, acc = 0.86529\n","[ Valid | 152/500 ] loss = 0.75627, acc = 0.78455\n","[ Train | 153/500 ] loss = 0.67829, acc = 0.87340\n","[ Valid | 153/500 ] loss = 0.75129, acc = 0.78251\n","[ Train | 154/500 ] loss = 0.67705, acc = 0.87209\n","[ Valid | 154/500 ] loss = 0.75278, acc = 0.78192\n","[ Train | 155/500 ] loss = 0.67711, acc = 0.87097\n","[ Valid | 155/500 ] loss = 0.75202, acc = 0.78980\n","[ Train | 156/500 ] loss = 0.67631, acc = 0.87655\n","[ Valid | 156/500 ] loss = 0.74884, acc = 0.79913\n","[ Train | 157/500 ] loss = 0.67413, acc = 0.87482\n","[ Valid | 157/500 ] loss = 0.75514, acc = 0.77959\n","[ Train | 158/500 ] loss = 0.67146, acc = 0.87746\n","[ Valid | 158/500 ] loss = 0.75520, acc = 0.77638\n","[ Train | 159/500 ] loss = 0.67762, acc = 0.87381\n","[ Valid | 159/500 ] loss = 0.75602, acc = 0.78834\n","[ Train | 160/500 ] loss = 0.67412, acc = 0.87391\n","[ Valid | 160/500 ] loss = 0.75643, acc = 0.78192\n","[ Train | 161/500 ] loss = 0.67285, acc = 0.87726\n","[ Valid | 161/500 ] loss = 0.75579, acc = 0.78542\n","[ Train | 162/500 ] loss = 0.67296, acc = 0.88090\n","[ Valid | 162/500 ] loss = 0.75149, acc = 0.78192\n","[ Train | 163/500 ] loss = 0.67161, acc = 0.87959\n","[ Valid | 163/500 ] loss = 0.75042, acc = 0.80204\n","[ Train | 164/500 ] loss = 0.67367, acc = 0.87492\n","[ Valid | 164/500 ] loss = 0.74981, acc = 0.79213\n","[ Train | 165/500 ] loss = 0.66900, acc = 0.88313\n","[ Valid | 165/500 ] loss = 0.75213, acc = 0.79504\n","[ Train | 166/500 ] loss = 0.67156, acc = 0.87807\n","[ Valid | 166/500 ] loss = 0.75343, acc = 0.79825\n","[ Train | 167/500 ] loss = 0.67141, acc = 0.87746\n","[ Valid | 167/500 ] loss = 0.75273, acc = 0.78805\n","[ Train | 168/500 ] loss = 0.67236, acc = 0.87807\n","[ Valid | 168/500 ] loss = 0.75113, acc = 0.79854\n","[ Train | 169/500 ] loss = 0.67638, acc = 0.87655\n","[ Valid | 169/500 ] loss = 0.75389, acc = 0.78542\n","[ Train | 170/500 ] loss = 0.67082, acc = 0.87979\n","[ Valid | 170/500 ] loss = 0.75267, acc = 0.78921\n","[ Train | 171/500 ] loss = 0.67530, acc = 0.87898\n","[ Valid | 171/500 ] loss = 0.75013, acc = 0.78980\n","[ Train | 172/500 ] loss = 0.67357, acc = 0.87807\n","[ Valid | 172/500 ] loss = 0.75166, acc = 0.79067\n","[ Train | 173/500 ] loss = 0.67271, acc = 0.88050\n","[ Valid | 173/500 ] loss = 0.75474, acc = 0.77201\n","[ Train | 174/500 ] loss = 0.67074, acc = 0.87705\n","[ Valid | 174/500 ] loss = 0.75195, acc = 0.78367\n","[ Train | 175/500 ] loss = 0.67496, acc = 0.87472\n","[ Valid | 175/500 ] loss = 0.74817, acc = 0.79417\n","[ Train | 176/500 ] loss = 0.67064, acc = 0.88090\n","[ Valid | 176/500 ] loss = 0.75069, acc = 0.79475\n","[ Train | 177/500 ] loss = 0.67322, acc = 0.87573\n","[ Valid | 177/500 ] loss = 0.75512, acc = 0.78542\n","[ Train | 178/500 ] loss = 0.67248, acc = 0.87340\n","[ Valid | 178/500 ] loss = 0.74961, acc = 0.79009\n","[ Train | 179/500 ] loss = 0.66868, acc = 0.88232\n","[ Valid | 179/500 ] loss = 0.75660, acc = 0.78047\n","[ Train | 180/500 ] loss = 0.67421, acc = 0.87705\n","[ Valid | 180/500 ] loss = 0.75181, acc = 0.79213\n","[ Train | 181/500 ] loss = 0.67219, acc = 0.87837\n","[ Valid | 181/500 ] loss = 0.75192, acc = 0.78426\n","[ Train | 182/500 ] loss = 0.67382, acc = 0.87665\n","[ Valid | 182/500 ] loss = 0.75551, acc = 0.79067\n","[ Train | 183/500 ] loss = 0.67442, acc = 0.87482\n","[ Valid | 183/500 ] loss = 0.75088, acc = 0.78746\n","[ Train | 184/500 ] loss = 0.67156, acc = 0.87786\n","[ Valid | 184/500 ] loss = 0.75235, acc = 0.78776\n","[ Train | 185/500 ] loss = 0.67255, acc = 0.87573\n","[ Valid | 185/500 ] loss = 0.75165, acc = 0.79213\n","[ Train | 186/500 ] loss = 0.67028, acc = 0.87938\n","[ Valid | 186/500 ] loss = 0.74829, acc = 0.78921\n","[ Train | 187/500 ] loss = 0.67005, acc = 0.88009\n","[ Valid | 187/500 ] loss = 0.74887, acc = 0.79096\n","[ Train | 188/500 ] loss = 0.67171, acc = 0.87938\n","[ Valid | 188/500 ] loss = 0.75296, acc = 0.78892\n","[ Train | 189/500 ] loss = 0.67300, acc = 0.87523\n","[ Valid | 189/500 ] loss = 0.75398, acc = 0.78746\n","[ Train | 190/500 ] loss = 0.67159, acc = 0.87705\n","[ Valid | 190/500 ] loss = 0.74906, acc = 0.79155\n","[ Train | 191/500 ] loss = 0.67103, acc = 0.88090\n","[ Valid | 191/500 ] loss = 0.75085, acc = 0.79563\n","[ Train | 192/500 ] loss = 0.67248, acc = 0.88040\n","[ Valid | 192/500 ] loss = 0.75067, acc = 0.79155\n","[ Train | 193/500 ] loss = 0.67168, acc = 0.88090\n","[ Valid | 193/500 ] loss = 0.75075, acc = 0.78513\n","[ Train | 194/500 ] loss = 0.67251, acc = 0.87746\n","[ Valid | 194/500 ] loss = 0.75388, acc = 0.78367\n","[ Train | 195/500 ] loss = 0.67224, acc = 0.87908\n","[ Valid | 195/500 ] loss = 0.75258, acc = 0.78950\n","[ Train | 196/500 ] loss = 0.66929, acc = 0.88131\n","[ Valid | 196/500 ] loss = 0.75243, acc = 0.79096\n","[ Train | 197/500 ] loss = 0.66970, acc = 0.88182\n","[ Valid | 197/500 ] loss = 0.75304, acc = 0.79913\n","[ Train | 198/500 ] loss = 0.67084, acc = 0.88192\n","[ Valid | 198/500 ] loss = 0.75025, acc = 0.79300\n","[ Train | 199/500 ] loss = 0.66994, acc = 0.87796\n","[ Valid | 199/500 ] loss = 0.75074, acc = 0.79913\n","[ Train | 200/500 ] loss = 0.67068, acc = 0.87928\n","[ Valid | 200/500 ] loss = 0.75701, acc = 0.78105\n","[ Train | 201/500 ] loss = 0.66454, acc = 0.88384\n","[ Valid | 201/500 ] loss = 0.75078, acc = 0.78280\n","[ Train | 202/500 ] loss = 0.66539, acc = 0.88415\n","[ Valid | 202/500 ] loss = 0.74768, acc = 0.78921\n","[ Train | 203/500 ] loss = 0.66523, acc = 0.88080\n","[ Valid | 203/500 ] loss = 0.75000, acc = 0.78426\n","[ Train | 204/500 ] loss = 0.66334, acc = 0.88830\n","[ Valid | 204/500 ] loss = 0.74914, acc = 0.79942\n","[ Train | 205/500 ] loss = 0.66499, acc = 0.88242\n","[ Valid | 205/500 ] loss = 0.74782, acc = 0.78746\n","[ Train | 206/500 ] loss = 0.66152, acc = 0.88932\n","[ Valid | 206/500 ] loss = 0.74704, acc = 0.78717\n","[ Train | 207/500 ] loss = 0.66120, acc = 0.88759\n","[ Valid | 207/500 ] loss = 0.74897, acc = 0.78746\n","[ Train | 208/500 ] loss = 0.66669, acc = 0.88131\n","[ Valid | 208/500 ] loss = 0.74867, acc = 0.78746\n","[ Train | 209/500 ] loss = 0.66742, acc = 0.88465\n","[ Valid | 209/500 ] loss = 0.74974, acc = 0.79271\n","[ Train | 210/500 ] loss = 0.66174, acc = 0.88962\n","[ Valid | 210/500 ] loss = 0.74483, acc = 0.80000\n","[ Train | 211/500 ] loss = 0.66444, acc = 0.88465\n","[ Valid | 211/500 ] loss = 0.74742, acc = 0.78688\n","[ Train | 212/500 ] loss = 0.66178, acc = 0.89327\n","[ Valid | 212/500 ] loss = 0.74567, acc = 0.79038\n","[ Train | 213/500 ] loss = 0.66337, acc = 0.88516\n","[ Valid | 213/500 ] loss = 0.74651, acc = 0.78950\n","[ Train | 214/500 ] loss = 0.66226, acc = 0.88617\n","[ Valid | 214/500 ] loss = 0.74725, acc = 0.78921\n","[ Train | 215/500 ] loss = 0.66290, acc = 0.88688\n","[ Valid | 215/500 ] loss = 0.74646, acc = 0.78688\n","[ Train | 216/500 ] loss = 0.66189, acc = 0.88688\n","[ Valid | 216/500 ] loss = 0.74534, acc = 0.78980\n","[ Train | 217/500 ] loss = 0.66716, acc = 0.88394\n","[ Valid | 217/500 ] loss = 0.74715, acc = 0.79067\n","[ Train | 218/500 ] loss = 0.66230, acc = 0.88607\n","[ Valid | 218/500 ] loss = 0.75034, acc = 0.78776\n","[ Train | 219/500 ] loss = 0.66481, acc = 0.88749\n","[ Valid | 219/500 ] loss = 0.74751, acc = 0.79446\n","[ Train | 220/500 ] loss = 0.66427, acc = 0.88344\n","[ Valid | 220/500 ] loss = 0.74712, acc = 0.79621\n","[ Train | 221/500 ] loss = 0.65965, acc = 0.88749\n","[ Valid | 221/500 ] loss = 0.74658, acc = 0.78950\n","[ Train | 222/500 ] loss = 0.66179, acc = 0.88810\n","[ Valid | 222/500 ] loss = 0.74934, acc = 0.79096\n","[ Train | 223/500 ] loss = 0.66157, acc = 0.88810\n","[ Valid | 223/500 ] loss = 0.74870, acc = 0.78805\n","[ Train | 224/500 ] loss = 0.66279, acc = 0.88861\n","[ Valid | 224/500 ] loss = 0.74963, acc = 0.78601\n","[ Train | 225/500 ] loss = 0.66209, acc = 0.88476\n","[ Valid | 225/500 ] loss = 0.74598, acc = 0.78776\n","[ Train | 226/500 ] loss = 0.65871, acc = 0.89226\n","[ Valid | 226/500 ] loss = 0.74886, acc = 0.78776\n","[ Train | 227/500 ] loss = 0.65921, acc = 0.89084\n","[ Valid | 227/500 ] loss = 0.74873, acc = 0.78630\n","[ Train | 228/500 ] loss = 0.66306, acc = 0.88729\n","[ Valid | 228/500 ] loss = 0.74688, acc = 0.78950\n","[ Train | 229/500 ] loss = 0.66381, acc = 0.88455\n","[ Valid | 229/500 ] loss = 0.75050, acc = 0.79300\n","[ Train | 230/500 ] loss = 0.66191, acc = 0.89134\n","[ Valid | 230/500 ] loss = 0.74887, acc = 0.79329\n","[ Train | 231/500 ] loss = 0.66234, acc = 0.88719\n","[ Valid | 231/500 ] loss = 0.74805, acc = 0.79417\n","[ Train | 232/500 ] loss = 0.66006, acc = 0.89175\n","[ Valid | 232/500 ] loss = 0.74731, acc = 0.78659\n","[ Train | 233/500 ] loss = 0.66023, acc = 0.88739\n","[ Valid | 233/500 ] loss = 0.74928, acc = 0.78688\n","[ Train | 234/500 ] loss = 0.66155, acc = 0.88911\n","[ Valid | 234/500 ] loss = 0.75038, acc = 0.79446\n","[ Train | 235/500 ] loss = 0.66198, acc = 0.89357\n","[ Valid | 235/500 ] loss = 0.74892, acc = 0.78717\n","[ Train | 236/500 ] loss = 0.66288, acc = 0.88607\n","[ Valid | 236/500 ] loss = 0.74874, acc = 0.79534\n","[ Train | 237/500 ] loss = 0.65949, acc = 0.89053\n","[ Valid | 237/500 ] loss = 0.74928, acc = 0.79067\n","[ Train | 238/500 ] loss = 0.66019, acc = 0.88972\n","[ Valid | 238/500 ] loss = 0.74735, acc = 0.79009\n","[ Train | 239/500 ] loss = 0.66355, acc = 0.89043\n","[ Valid | 239/500 ] loss = 0.74712, acc = 0.78776\n","[ Train | 240/500 ] loss = 0.65976, acc = 0.89215\n","[ Valid | 240/500 ] loss = 0.75029, acc = 0.79038\n","[ Train | 241/500 ] loss = 0.66339, acc = 0.88922\n","[ Valid | 241/500 ] loss = 0.75003, acc = 0.78251\n","[ Train | 242/500 ] loss = 0.66088, acc = 0.88932\n","[ Valid | 242/500 ] loss = 0.74803, acc = 0.79563\n","[ Train | 243/500 ] loss = 0.65917, acc = 0.88749\n","[ Valid | 243/500 ] loss = 0.74747, acc = 0.78921\n","[ Train | 244/500 ] loss = 0.66049, acc = 0.88982\n","[ Valid | 244/500 ] loss = 0.74819, acc = 0.79155\n","[ Train | 245/500 ] loss = 0.66006, acc = 0.89297\n","[ Valid | 245/500 ] loss = 0.74623, acc = 0.79738\n","[ Train | 246/500 ] loss = 0.66294, acc = 0.89023\n","[ Valid | 246/500 ] loss = 0.74873, acc = 0.79359\n","[ Train | 247/500 ] loss = 0.66172, acc = 0.88668\n","[ Valid | 247/500 ] loss = 0.74672, acc = 0.79184\n","[ Train | 248/500 ] loss = 0.66080, acc = 0.89236\n","[ Valid | 248/500 ] loss = 0.75159, acc = 0.78105\n","[ Train | 249/500 ] loss = 0.65748, acc = 0.89215\n","[ Valid | 249/500 ] loss = 0.74801, acc = 0.78892\n","[ Train | 250/500 ] loss = 0.66183, acc = 0.88678\n","[ Valid | 250/500 ] loss = 0.74742, acc = 0.78338\n","[ Train | 251/500 ] loss = 0.65616, acc = 0.89398\n","[ Valid | 251/500 ] loss = 0.74979, acc = 0.78076\n","[ Train | 252/500 ] loss = 0.65820, acc = 0.88658\n","[ Valid | 252/500 ] loss = 0.74671, acc = 0.78834\n","[ Train | 253/500 ] loss = 0.66081, acc = 0.88952\n","[ Valid | 253/500 ] loss = 0.74828, acc = 0.79213\n","[ Train | 254/500 ] loss = 0.65827, acc = 0.89033\n","[ Valid | 254/500 ] loss = 0.74709, acc = 0.78805\n","[ Train | 255/500 ] loss = 0.65738, acc = 0.89337\n","[ Valid | 255/500 ] loss = 0.74638, acc = 0.78601\n","[ Train | 256/500 ] loss = 0.65730, acc = 0.89023\n","[ Valid | 256/500 ] loss = 0.74768, acc = 0.78892\n","[ Train | 257/500 ] loss = 0.65610, acc = 0.89641\n","[ Valid | 257/500 ] loss = 0.74480, acc = 0.78980\n","[ Train | 258/500 ] loss = 0.65606, acc = 0.89043\n","[ Valid | 258/500 ] loss = 0.74626, acc = 0.78950\n","[ Train | 259/500 ] loss = 0.65723, acc = 0.89063\n","[ Valid | 259/500 ] loss = 0.74447, acc = 0.78834\n","[ Train | 260/500 ] loss = 0.65618, acc = 0.89205\n","[ Valid | 260/500 ] loss = 0.74681, acc = 0.78659\n","[ Train | 261/500 ] loss = 0.65679, acc = 0.89043\n","[ Valid | 261/500 ] loss = 0.74541, acc = 0.78805\n","[ Train | 262/500 ] loss = 0.66158, acc = 0.88800\n","[ Valid | 262/500 ] loss = 0.74521, acc = 0.78950\n","[ Train | 263/500 ] loss = 0.65546, acc = 0.89641\n","[ Valid | 263/500 ] loss = 0.74679, acc = 0.78980\n","[ Train | 264/500 ] loss = 0.65945, acc = 0.88840\n","[ Valid | 264/500 ] loss = 0.74684, acc = 0.78776\n","[ Train | 265/500 ] loss = 0.65941, acc = 0.88699\n","[ Valid | 265/500 ] loss = 0.74547, acc = 0.78805\n","[ Train | 266/500 ] loss = 0.65301, acc = 0.89793\n","[ Valid | 266/500 ] loss = 0.74655, acc = 0.79854\n","[ Train | 267/500 ] loss = 0.65899, acc = 0.89104\n","[ Valid | 267/500 ] loss = 0.74774, acc = 0.79329\n","[ Train | 268/500 ] loss = 0.65477, acc = 0.89398\n","[ Valid | 268/500 ] loss = 0.74757, acc = 0.79009\n","[ Train | 269/500 ] loss = 0.65779, acc = 0.89175\n","[ Valid | 269/500 ] loss = 0.74743, acc = 0.78921\n","[ Train | 270/500 ] loss = 0.65762, acc = 0.88881\n","[ Valid | 270/500 ] loss = 0.74725, acc = 0.78863\n","[ Train | 271/500 ] loss = 0.65316, acc = 0.89530\n","[ Valid | 271/500 ] loss = 0.74616, acc = 0.78659\n","[ Train | 272/500 ] loss = 0.65764, acc = 0.89530\n","[ Valid | 272/500 ] loss = 0.74565, acc = 0.79125\n","[ Train | 273/500 ] loss = 0.65177, acc = 0.89834\n","[ Valid | 273/500 ] loss = 0.74993, acc = 0.78601\n","[ Train | 274/500 ] loss = 0.65987, acc = 0.88861\n","[ Valid | 274/500 ] loss = 0.74443, acc = 0.78805\n","[ Train | 275/500 ] loss = 0.65520, acc = 0.89449\n","[ Valid | 275/500 ] loss = 0.74858, acc = 0.78892\n","[ Train | 276/500 ] loss = 0.65715, acc = 0.89378\n","[ Valid | 276/500 ] loss = 0.74728, acc = 0.78892\n","[ Train | 277/500 ] loss = 0.65903, acc = 0.89033\n","[ Valid | 277/500 ] loss = 0.74674, acc = 0.78455\n","[ Train | 278/500 ] loss = 0.65916, acc = 0.89398\n","[ Valid | 278/500 ] loss = 0.74821, acc = 0.78717\n","[ Train | 279/500 ] loss = 0.65777, acc = 0.89347\n","[ Valid | 279/500 ] loss = 0.74808, acc = 0.78980\n","[ Train | 280/500 ] loss = 0.65766, acc = 0.89317\n","[ Valid | 280/500 ] loss = 0.74769, acc = 0.78950\n","[ Train | 281/500 ] loss = 0.65614, acc = 0.89134\n","[ Valid | 281/500 ] loss = 0.74744, acc = 0.78396\n","[ Train | 282/500 ] loss = 0.65652, acc = 0.89469\n","[ Valid | 282/500 ] loss = 0.74681, acc = 0.79242\n","[ Train | 283/500 ] loss = 0.65317, acc = 0.89672\n","[ Valid | 283/500 ] loss = 0.74994, acc = 0.78017\n","[ Train | 284/500 ] loss = 0.65790, acc = 0.89094\n","[ Valid | 284/500 ] loss = 0.74544, acc = 0.78426\n","[ Train | 285/500 ] loss = 0.65794, acc = 0.89276\n","[ Valid | 285/500 ] loss = 0.74684, acc = 0.79009\n","[ Train | 286/500 ] loss = 0.65575, acc = 0.89205\n","[ Valid | 286/500 ] loss = 0.74800, acc = 0.78950\n","[ Train | 287/500 ] loss = 0.65343, acc = 0.89266\n","[ Valid | 287/500 ] loss = 0.74620, acc = 0.78396\n","[ Train | 288/500 ] loss = 0.65716, acc = 0.89165\n","[ Valid | 288/500 ] loss = 0.74732, acc = 0.78163\n","[ Train | 289/500 ] loss = 0.65729, acc = 0.89175\n","[ Valid | 289/500 ] loss = 0.74639, acc = 0.78805\n","[ Train | 290/500 ] loss = 0.65579, acc = 0.89550\n","[ Valid | 290/500 ] loss = 0.74741, acc = 0.79184\n","[ Train | 291/500 ] loss = 0.65638, acc = 0.89114\n","[ Valid | 291/500 ] loss = 0.75053, acc = 0.77872\n","[ Train | 292/500 ] loss = 0.65569, acc = 0.89337\n","[ Valid | 292/500 ] loss = 0.74698, acc = 0.78863\n","[ Train | 293/500 ] loss = 0.65923, acc = 0.88982\n","[ Valid | 293/500 ] loss = 0.74728, acc = 0.79738\n","[ Train | 294/500 ] loss = 0.65590, acc = 0.89266\n","[ Valid | 294/500 ] loss = 0.74552, acc = 0.78542\n","[ Train | 295/500 ] loss = 0.65320, acc = 0.89722\n","[ Valid | 295/500 ] loss = 0.74705, acc = 0.78892\n","[ Train | 296/500 ] loss = 0.65917, acc = 0.88861\n","[ Valid | 296/500 ] loss = 0.74945, acc = 0.78601\n","[ Train | 297/500 ] loss = 0.65172, acc = 0.89773\n","[ Valid | 297/500 ] loss = 0.74673, acc = 0.78426\n","[ Train | 298/500 ] loss = 0.65657, acc = 0.89195\n","[ Valid | 298/500 ] loss = 0.74811, acc = 0.78892\n","[ Train | 299/500 ] loss = 0.65438, acc = 0.89824\n","[ Valid | 299/500 ] loss = 0.74609, acc = 0.78834\n","[ Train | 300/500 ] loss = 0.65557, acc = 0.89327\n","[ Valid | 300/500 ] loss = 0.74604, acc = 0.79067\n","[ Train | 301/500 ] loss = 0.65523, acc = 0.89641\n","[ Valid | 301/500 ] loss = 0.74573, acc = 0.78892\n","[ Train | 302/500 ] loss = 0.65286, acc = 0.89641\n","[ Valid | 302/500 ] loss = 0.74737, acc = 0.78396\n","[ Train | 303/500 ] loss = 0.65386, acc = 0.89672\n","[ Valid | 303/500 ] loss = 0.74698, acc = 0.78367\n","[ Train | 304/500 ] loss = 0.65433, acc = 0.89438\n","[ Valid | 304/500 ] loss = 0.74737, acc = 0.79038\n","[ Train | 305/500 ] loss = 0.65187, acc = 0.89793\n","[ Valid | 305/500 ] loss = 0.74784, acc = 0.78630\n","[ Train | 306/500 ] loss = 0.65375, acc = 0.89357\n","[ Valid | 306/500 ] loss = 0.74629, acc = 0.78863\n","[ Train | 307/500 ] loss = 0.65496, acc = 0.89398\n","[ Valid | 307/500 ] loss = 0.74571, acc = 0.78950\n","[ Train | 308/500 ] loss = 0.65539, acc = 0.89286\n","[ Valid | 308/500 ] loss = 0.74624, acc = 0.79271\n","[ Train | 309/500 ] loss = 0.65136, acc = 0.89844\n","[ Valid | 309/500 ] loss = 0.74426, acc = 0.79621\n","[ Train | 310/500 ] loss = 0.65236, acc = 0.89449\n","[ Valid | 310/500 ] loss = 0.74653, acc = 0.78105\n","[ Train | 311/500 ] loss = 0.65463, acc = 0.89388\n","[ Valid | 311/500 ] loss = 0.74585, acc = 0.79388\n","[ Train | 312/500 ] loss = 0.65597, acc = 0.89155\n","[ Valid | 312/500 ] loss = 0.74711, acc = 0.78513\n","[ Train | 313/500 ] loss = 0.65558, acc = 0.88972\n","[ Valid | 313/500 ] loss = 0.74528, acc = 0.78484\n","[ Train | 314/500 ] loss = 0.65393, acc = 0.89844\n","[ Valid | 314/500 ] loss = 0.74650, acc = 0.79067\n","[ Train | 315/500 ] loss = 0.65171, acc = 0.89854\n","[ Valid | 315/500 ] loss = 0.74383, acc = 0.78921\n","[ Train | 316/500 ] loss = 0.65285, acc = 0.89621\n","[ Valid | 316/500 ] loss = 0.74681, acc = 0.78892\n","[ Train | 317/500 ] loss = 0.65447, acc = 0.89499\n","[ Valid | 317/500 ] loss = 0.74638, acc = 0.78659\n","[ Train | 318/500 ] loss = 0.65520, acc = 0.89428\n","[ Valid | 318/500 ] loss = 0.74613, acc = 0.79096\n","[ Train | 319/500 ] loss = 0.65529, acc = 0.89165\n","[ Valid | 319/500 ] loss = 0.74433, acc = 0.79271\n","[ Train | 320/500 ] loss = 0.64909, acc = 0.90036\n","[ Valid | 320/500 ] loss = 0.74541, acc = 0.78950\n","[ Train | 321/500 ] loss = 0.65465, acc = 0.89540\n","[ Valid | 321/500 ] loss = 0.74638, acc = 0.78513\n","[ Train | 322/500 ] loss = 0.65194, acc = 0.89732\n","[ Valid | 322/500 ] loss = 0.74661, acc = 0.79038\n","[ Train | 323/500 ] loss = 0.65374, acc = 0.89276\n","[ Valid | 323/500 ] loss = 0.74720, acc = 0.78542\n","[ Train | 324/500 ] loss = 0.65669, acc = 0.89428\n","[ Valid | 324/500 ] loss = 0.74451, acc = 0.79359\n","[ Train | 325/500 ] loss = 0.65120, acc = 0.89601\n","[ Valid | 325/500 ] loss = 0.74405, acc = 0.79125\n","[ Train | 326/500 ] loss = 0.65295, acc = 0.89074\n","[ Valid | 326/500 ] loss = 0.74671, acc = 0.78659\n","[ Train | 327/500 ] loss = 0.65228, acc = 0.89509\n","[ Valid | 327/500 ] loss = 0.74712, acc = 0.78717\n","[ Train | 328/500 ] loss = 0.65603, acc = 0.89215\n","[ Valid | 328/500 ] loss = 0.74620, acc = 0.78921\n","[ Train | 329/500 ] loss = 0.65457, acc = 0.89398\n","[ Valid | 329/500 ] loss = 0.74523, acc = 0.78921\n","[ Train | 330/500 ] loss = 0.65343, acc = 0.89459\n","[ Valid | 330/500 ] loss = 0.74460, acc = 0.78950\n","[ Train | 331/500 ] loss = 0.65408, acc = 0.89540\n","[ Valid | 331/500 ] loss = 0.74551, acc = 0.78571\n","[ Train | 332/500 ] loss = 0.65685, acc = 0.89378\n","[ Valid | 332/500 ] loss = 0.74844, acc = 0.78280\n","[ Train | 333/500 ] loss = 0.65429, acc = 0.89449\n","[ Valid | 333/500 ] loss = 0.74489, acc = 0.78746\n","[ Train | 334/500 ] loss = 0.65178, acc = 0.89560\n","[ Valid | 334/500 ] loss = 0.74517, acc = 0.78834\n","[ Train | 335/500 ] loss = 0.65160, acc = 0.89783\n","[ Valid | 335/500 ] loss = 0.74418, acc = 0.78484\n","[ Train | 336/500 ] loss = 0.64976, acc = 0.90057\n","[ Valid | 336/500 ] loss = 0.74446, acc = 0.79854\n","[ Train | 337/500 ] loss = 0.65192, acc = 0.89935\n","[ Valid | 337/500 ] loss = 0.74623, acc = 0.78513\n","[ Train | 338/500 ] loss = 0.65362, acc = 0.89428\n","[ Valid | 338/500 ] loss = 0.74692, acc = 0.78367\n","[ Train | 339/500 ] loss = 0.65319, acc = 0.89712\n","[ Valid | 339/500 ] loss = 0.74396, acc = 0.79155\n","[ Train | 340/500 ] loss = 0.65330, acc = 0.89682\n","[ Valid | 340/500 ] loss = 0.74473, acc = 0.79184\n","[ Train | 341/500 ] loss = 0.65374, acc = 0.89145\n","[ Valid | 341/500 ] loss = 0.74654, acc = 0.78601\n","[ Train | 342/500 ] loss = 0.64996, acc = 0.90280\n","[ Valid | 342/500 ] loss = 0.74821, acc = 0.78513\n","[ Train | 343/500 ] loss = 0.65414, acc = 0.89520\n","[ Valid | 343/500 ] loss = 0.74425, acc = 0.78980\n","[ Train | 344/500 ] loss = 0.65474, acc = 0.89560\n","[ Valid | 344/500 ] loss = 0.74553, acc = 0.78746\n","[ Train | 345/500 ] loss = 0.65483, acc = 0.89185\n","[ Valid | 345/500 ] loss = 0.74538, acc = 0.78863\n","[ Train | 346/500 ] loss = 0.65362, acc = 0.89783\n","[ Valid | 346/500 ] loss = 0.74388, acc = 0.79067\n","[ Train | 347/500 ] loss = 0.65189, acc = 0.89935\n","[ Valid | 347/500 ] loss = 0.74541, acc = 0.79125\n","[ Train | 348/500 ] loss = 0.65286, acc = 0.89712\n","[ Valid | 348/500 ] loss = 0.74533, acc = 0.78659\n","[ Train | 349/500 ] loss = 0.65320, acc = 0.89591\n","[ Valid | 349/500 ] loss = 0.74455, acc = 0.79184\n","[ Train | 350/500 ] loss = 0.65113, acc = 0.89499\n","[ Valid | 350/500 ] loss = 0.74822, acc = 0.79038\n","[ Train | 351/500 ] loss = 0.65072, acc = 0.89591\n","[ Valid | 351/500 ] loss = 0.74490, acc = 0.79475\n","[ Train | 352/500 ] loss = 0.65253, acc = 0.89814\n","[ Valid | 352/500 ] loss = 0.74500, acc = 0.79213\n","[ Train | 353/500 ] loss = 0.65106, acc = 0.89925\n","[ Valid | 353/500 ] loss = 0.74635, acc = 0.79125\n","[ Train | 354/500 ] loss = 0.65247, acc = 0.89783\n","[ Valid | 354/500 ] loss = 0.74364, acc = 0.78863\n","[ Train | 355/500 ] loss = 0.65294, acc = 0.89824\n","[ Valid | 355/500 ] loss = 0.74485, acc = 0.79300\n","[ Train | 356/500 ] loss = 0.65378, acc = 0.89337\n","[ Valid | 356/500 ] loss = 0.74373, acc = 0.79329\n","[ Train | 357/500 ] loss = 0.65138, acc = 0.89834\n","[ Valid | 357/500 ] loss = 0.74403, acc = 0.79388\n","[ Train | 358/500 ] loss = 0.65219, acc = 0.89530\n","[ Valid | 358/500 ] loss = 0.74348, acc = 0.79125\n","[ Train | 359/500 ] loss = 0.65214, acc = 0.89469\n","[ Valid | 359/500 ] loss = 0.74257, acc = 0.79155\n","[ Train | 360/500 ] loss = 0.65124, acc = 0.89631\n","[ Valid | 360/500 ] loss = 0.74636, acc = 0.78746\n","[ Train | 361/500 ] loss = 0.65123, acc = 0.89915\n","[ Valid | 361/500 ] loss = 0.74649, acc = 0.79563\n","[ Train | 362/500 ] loss = 0.65155, acc = 0.89631\n","[ Valid | 362/500 ] loss = 0.74366, acc = 0.79038\n","[ Train | 363/500 ] loss = 0.65216, acc = 0.89672\n","[ Valid | 363/500 ] loss = 0.74704, acc = 0.78484\n","[ Train | 364/500 ] loss = 0.65302, acc = 0.89368\n","[ Valid | 364/500 ] loss = 0.74470, acc = 0.78542\n","[ Train | 365/500 ] loss = 0.65233, acc = 0.89803\n","[ Valid | 365/500 ] loss = 0.74482, acc = 0.79213\n","[ Train | 366/500 ] loss = 0.64948, acc = 0.90077\n","[ Valid | 366/500 ] loss = 0.74378, acc = 0.79417\n","[ Train | 367/500 ] loss = 0.64915, acc = 0.90016\n","[ Valid | 367/500 ] loss = 0.74622, acc = 0.78571\n","[ Train | 368/500 ] loss = 0.65122, acc = 0.89591\n","[ Valid | 368/500 ] loss = 0.74449, acc = 0.78950\n","[ Train | 369/500 ] loss = 0.65517, acc = 0.89337\n","[ Valid | 369/500 ] loss = 0.74561, acc = 0.78980\n","[ Train | 370/500 ] loss = 0.65308, acc = 0.89550\n","[ Valid | 370/500 ] loss = 0.74396, acc = 0.79592\n","[ Train | 371/500 ] loss = 0.65337, acc = 0.89570\n","[ Valid | 371/500 ] loss = 0.74512, acc = 0.78542\n","[ Train | 372/500 ] loss = 0.64993, acc = 0.89672\n","[ Valid | 372/500 ] loss = 0.74687, acc = 0.78892\n","[ Train | 373/500 ] loss = 0.65271, acc = 0.89611\n","[ Valid | 373/500 ] loss = 0.74396, acc = 0.78950\n","[ Train | 374/500 ] loss = 0.65203, acc = 0.89631\n","[ Valid | 374/500 ] loss = 0.74497, acc = 0.78950\n","[ Train | 375/500 ] loss = 0.65625, acc = 0.89368\n","[ Valid | 375/500 ] loss = 0.74642, acc = 0.79096\n","[ Train | 376/500 ] loss = 0.65604, acc = 0.89682\n","[ Valid | 376/500 ] loss = 0.74742, acc = 0.78338\n","[ Train | 377/500 ] loss = 0.65154, acc = 0.89814\n","[ Valid | 377/500 ] loss = 0.74501, acc = 0.79096\n","[ Train | 378/500 ] loss = 0.65013, acc = 0.90107\n","[ Valid | 378/500 ] loss = 0.74613, acc = 0.79096\n","[ Train | 379/500 ] loss = 0.65221, acc = 0.89874\n","[ Valid | 379/500 ] loss = 0.74446, acc = 0.78571\n","[ Train | 380/500 ] loss = 0.65089, acc = 0.90351\n","[ Valid | 380/500 ] loss = 0.74528, acc = 0.78950\n","[ Train | 381/500 ] loss = 0.65170, acc = 0.89783\n","[ Valid | 381/500 ] loss = 0.74384, acc = 0.78717\n","[ Train | 382/500 ] loss = 0.65242, acc = 0.89793\n","[ Valid | 382/500 ] loss = 0.74538, acc = 0.78426\n","[ Train | 383/500 ] loss = 0.65151, acc = 0.89702\n","[ Valid | 383/500 ] loss = 0.74453, acc = 0.79475\n","[ Train | 384/500 ] loss = 0.65274, acc = 0.89874\n","[ Valid | 384/500 ] loss = 0.74582, acc = 0.78688\n","[ Train | 385/500 ] loss = 0.65250, acc = 0.89793\n","[ Valid | 385/500 ] loss = 0.74613, acc = 0.78863\n","[ Train | 386/500 ] loss = 0.65185, acc = 0.89966\n","[ Valid | 386/500 ] loss = 0.74430, acc = 0.79796\n","[ Train | 387/500 ] loss = 0.65263, acc = 0.89114\n","[ Valid | 387/500 ] loss = 0.74595, acc = 0.78688\n","[ Train | 388/500 ] loss = 0.65116, acc = 0.89864\n","[ Valid | 388/500 ] loss = 0.74706, acc = 0.78601\n","[ Train | 389/500 ] loss = 0.64779, acc = 0.90107\n","[ Valid | 389/500 ] loss = 0.74537, acc = 0.78921\n","[ Train | 390/500 ] loss = 0.64843, acc = 0.90300\n","[ Valid | 390/500 ] loss = 0.74457, acc = 0.79650\n","[ Train | 391/500 ] loss = 0.65387, acc = 0.89428\n","[ Valid | 391/500 ] loss = 0.74370, acc = 0.79621\n","[ Train | 392/500 ] loss = 0.65389, acc = 0.89814\n","[ Valid | 392/500 ] loss = 0.74406, acc = 0.78630\n","[ Train | 393/500 ] loss = 0.64948, acc = 0.89722\n","[ Valid | 393/500 ] loss = 0.74605, acc = 0.78659\n","[ Train | 394/500 ] loss = 0.65012, acc = 0.90077\n","[ Valid | 394/500 ] loss = 0.74706, acc = 0.78921\n","[ Train | 395/500 ] loss = 0.65389, acc = 0.89317\n","[ Valid | 395/500 ] loss = 0.74386, acc = 0.79359\n","[ Train | 396/500 ] loss = 0.65240, acc = 0.89428\n","[ Valid | 396/500 ] loss = 0.74398, acc = 0.79213\n","[ Train | 397/500 ] loss = 0.65470, acc = 0.89388\n","[ Valid | 397/500 ] loss = 0.74457, acc = 0.79854\n","[ Train | 398/500 ] loss = 0.65268, acc = 0.89641\n","[ Valid | 398/500 ] loss = 0.74499, acc = 0.79155\n","[ Train | 399/500 ] loss = 0.65105, acc = 0.89803\n","[ Valid | 399/500 ] loss = 0.74652, acc = 0.79359\n","[ Train | 400/500 ] loss = 0.65355, acc = 0.89509\n","[ Valid | 400/500 ] loss = 0.74624, acc = 0.78892\n","[ Train | 401/500 ] loss = 0.65383, acc = 0.89398\n","[ Valid | 401/500 ] loss = 0.74499, acc = 0.78601\n","[ Train | 402/500 ] loss = 0.64869, acc = 0.90330\n","[ Valid | 402/500 ] loss = 0.74355, acc = 0.79446\n","[ Train | 403/500 ] loss = 0.65106, acc = 0.89803\n","[ Valid | 403/500 ] loss = 0.74651, acc = 0.78834\n","[ Train | 404/500 ] loss = 0.65101, acc = 0.89712\n","[ Valid | 404/500 ] loss = 0.74410, acc = 0.78746\n","[ Train | 405/500 ] loss = 0.65120, acc = 0.89601\n","[ Valid | 405/500 ] loss = 0.74615, acc = 0.78863\n","[ Train | 406/500 ] loss = 0.65380, acc = 0.89256\n","[ Valid | 406/500 ] loss = 0.74261, acc = 0.79329\n","[ Train | 407/500 ] loss = 0.64845, acc = 0.89986\n","[ Valid | 407/500 ] loss = 0.74782, acc = 0.77988\n","[ Train | 408/500 ] loss = 0.65272, acc = 0.89580\n","[ Valid | 408/500 ] loss = 0.74578, acc = 0.78309\n","[ Train | 409/500 ] loss = 0.65348, acc = 0.89601\n","[ Valid | 409/500 ] loss = 0.74632, acc = 0.79300\n","[ Train | 410/500 ] loss = 0.65227, acc = 0.89844\n","[ Valid | 410/500 ] loss = 0.74530, acc = 0.78980\n","[ Train | 411/500 ] loss = 0.65076, acc = 0.90219\n","[ Valid | 411/500 ] loss = 0.74542, acc = 0.79125\n","[ Train | 412/500 ] loss = 0.64795, acc = 0.90158\n","[ Valid | 412/500 ] loss = 0.74564, acc = 0.78542\n","[ Train | 413/500 ] loss = 0.65181, acc = 0.89763\n","[ Valid | 413/500 ] loss = 0.74406, acc = 0.79329\n","[ Train | 414/500 ] loss = 0.64771, acc = 0.90249\n","[ Valid | 414/500 ] loss = 0.74494, acc = 0.79125\n","[ Train | 415/500 ] loss = 0.65152, acc = 0.89722\n","[ Valid | 415/500 ] loss = 0.74644, acc = 0.78396\n","[ Train | 416/500 ] loss = 0.64859, acc = 0.90351\n","[ Valid | 416/500 ] loss = 0.74561, acc = 0.78571\n","[ Train | 417/500 ] loss = 0.65046, acc = 0.89631\n","[ Valid | 417/500 ] loss = 0.74423, acc = 0.78892\n","[ Train | 418/500 ] loss = 0.65108, acc = 0.89438\n","[ Valid | 418/500 ] loss = 0.74411, acc = 0.79213\n","[ Train | 419/500 ] loss = 0.65127, acc = 0.89935\n","[ Valid | 419/500 ] loss = 0.74295, acc = 0.78921\n","[ Train | 420/500 ] loss = 0.65288, acc = 0.89702\n","[ Valid | 420/500 ] loss = 0.74670, acc = 0.78280\n","[ Train | 421/500 ] loss = 0.65085, acc = 0.89763\n","[ Valid | 421/500 ] loss = 0.74471, acc = 0.79913\n"]},{"ename":"KeyboardInterrupt","evalue":"","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_49769/3278282215.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0mtotal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m         \u001b[0;31m# A batch consists of image data and corresponding labels.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    529\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 530\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    531\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    532\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    568\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    569\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 570\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    571\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    572\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipykernel_49769/4216807747.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;31m# im = Image.open(fname)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0;31m# im = self.transform(im)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mim\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcvtColor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCOLOR_BGR2RGB\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Initialize a model, and put it on the device specified.\n","student_model.to(device)\n","# student_model.apply(reset_weights)\n","\n","# Initialize optimizer, you may fine-tune some hyperparameters such as learning rate on your own.\n","optimizer = torch.optim.Adam(student_model.parameters(), lr=cfg['lr'], weight_decay=cfg['weight_decay']) \n","scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=50, gamma=0.5)\n","\n","# Initialize trackers, these are not parameters and should not be changed\n","stale = 0\n","best_acc = 0.0\n","\n","teacher_model.eval()  # MEDIUM BASELINE\n","for epoch in range(n_epochs):\n","\n","    # ---------- Training ----------\n","    # Make sure the model is in train mode before training.\n","    student_model.train()\n","\n","    # These are used to record information in training.\n","    train_loss = []\n","    train_accs = []\n","    train_lens = []\n","\n","    correct = 0\n","    total = 0\n","\n","    for batch in train_loader:\n","\n","        # A batch consists of image data and corresponding labels.\n","        imgs, labels = batch\n","        imgs = imgs.to(device)\n","        labels = labels.to(device)\n","\n","        inputs, targets_a, targets_b, lam = mixup_data(imgs, labels)\n","        inputs, targets_a, targets_b = map(Variable, (inputs, targets_a, targets_b))\n","            \n","\n","        # Forward the data. (Make sure data and model are on the same device.)\n","        with torch.no_grad():  # MEDIUM BASELINE\n","            teacher_logits = teacher_model(imgs)  # MEDIUM BASELINE\n","        \n","        logits = student_model(imgs)\n","\n","        # Calculate the cross-entropy loss.\n","        # We don't need to apply softmax before computing cross-entropy as it is done automatically.\n","        loss = loss_fn(logits, labels, teacher_logits) # MEDIUM BASELINE\n","        # loss = loss_fn(logits, labels) # SIMPLE BASELINE\n","        # loss = mixup_criterion(loss_fn, logits, teacher_logits, targets_a, targets_b, lam)\n","        # Gradients stored in the parameters in the previous step should be cleared out first.\n","        optimizer.zero_grad()\n","\n","        # Compute the gradients for parameters.\n","        loss.backward()\n","\n","        # Clip the gradient norms for stable training.\n","        grad_norm = nn.utils.clip_grad_norm_(student_model.parameters(), max_norm=cfg['grad_norm_max'])\n","\n","        # Update the parameters with computed gradients.\n","        optimizer.step()\n","\n","        # Compute the accuracy for current batch.\n","        acc = (logits.argmax(dim=-1) == labels).float().sum()\n","        # _, predicted = torch.max(logits.data, 1)\n","        # correct += (lam * predicted.eq(targets_a.data).cpu().sum().float() + \n","        #             (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float())\n","        # total += labels.size(0)\n","\n","        # Record the loss and accuracy.\n","        train_batch_len = len(imgs)\n","        train_loss.append(loss.item() * train_batch_len)\n","        train_accs.append(acc)\n","        train_lens.append(train_batch_len)\n","        \n","    \n","    train_loss = sum(train_loss) / sum(train_lens)\n","    train_acc = sum(train_accs) / sum(train_lens)\n","    # train_acc = 100. * correct / total\n","\n","    # Print the information.\n","    log(f\"[ Train | {epoch + 1:03d}/{n_epochs:03d} ] loss = {train_loss:.5f}, acc = {train_acc:.5f}\")\n","\n","    # ---------- Validation ----------\n","    # Make sure the model is in eval mode so that some modules like dropout are disabled and work normally.\n","    student_model.eval()\n","\n","    # These are used to record information in validation.\n","    valid_loss = []\n","    valid_accs = []\n","    valid_lens = []\n","    \n","    total = 0\n","    correct = 0\n","\n","    # Iterate the validation set by batches.\n","    for batch in valid_loader:\n","\n","        # A batch consists of image data and corresponding labels.\n","        train_imgs, imgs, labels = batch\n","        imgs = imgs.to(device)\n","        labels = labels.to(device)\n","\n","        inputs, targets_a, targets_b, lam = mixup_data(imgs, labels)\n","        inputs, targets_a, targets_b = map(Variable, (inputs, targets_a, targets_b))\n","\n","        # We don't need gradient in validation.\n","        # Using torch.no_grad() accelerates the forward process.\n","        with torch.no_grad():\n","            logits = student_model(imgs)\n","            teacher_logits = teacher_model(imgs) # MEDIUM BASELINE\n","\n","        # We can still compute the loss (but not the gradient).\n","        loss = loss_fn(logits, labels, teacher_logits) # MEDIUM BASELINE\n","        # loss = loss_fn(logits, labels) # SIMPLE BASELINE\n","\n","        # Compute the accuracy for current batch.\n","        # _, predicted = torch.max(labels.data, 1)\n","        # correct += (lam * predicted.eq(targets_a.data).cpu().sum().float()\n","        #             + (1 - lam) * predicted.eq(targets_b.data).cpu().sum().float())\n","        # total += labels.size(0)\n","        acc = (logits.argmax(dim=-1) == labels).float().sum()\n","\n","        # Record the loss and accuracy.\n","        batch_len = len(imgs)\n","        valid_loss.append(loss.item() * batch_len)\n","        valid_accs.append(acc)\n","        valid_lens.append(batch_len)\n","        #break\n","\n","    # The average loss and accuracy for entire validation set is the average of the recorded values.\n","    valid_loss = sum(valid_loss) / sum(valid_lens)\n","    valid_acc = sum(valid_accs) / sum(valid_lens)\n","    # valid_acc = 100. * correct / total\n","\n","    # update logs\n","    \n","    if valid_acc > best_acc:\n","        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f} -> best\")\n","    else:\n","        log(f\"[ Valid | {epoch + 1:03d}/{n_epochs:03d} ] loss = {valid_loss:.5f}, acc = {valid_acc:.5f}\")\n","\n","\n","    # save models\n","    if valid_acc > best_acc:\n","        log(f\"Best model found at epoch {epoch}, saving model\")\n","        torch.save(student_model.state_dict(), f\"{save_path}/student_best.ckpt\") # only save best to prevent output memory exceed error\n","        best_acc = valid_acc\n","        stale = 0\n","    else:\n","        stale += 1\n","        if stale > patience:\n","            log(f\"No improvment {patience} consecutive epochs, early stopping\")\n","            break\n","    \n","    scheduler.step()\n","\n","log(\"Finish training\")\n","log_fw.close()"]},{"cell_type":"code","execution_count":66,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1655389082233,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"y0pIW_lRBmQn","outputId":"a11612bf-14a8-4100-db2a-ee96afa67a03"},"outputs":[{"data":{"text/plain":["143"]},"execution_count":66,"metadata":{},"output_type":"execute_result"}],"source":["import gc\n","del train_loader, valid_loader\n","gc.collect()"]},{"cell_type":"code","execution_count":67,"metadata":{"executionInfo":{"elapsed":417,"status":"ok","timestamp":1655389288017,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"anVb6jESzsWY"},"outputs":[],"source":["def TTA(model, train_ims, test_im):\n","    test_pred = model(test_im.to(device))\n","    train_pred = []\n","    for im in train_ims:\n","        train_pred.append(model(im.to(device)))\n","\n","    avg = sum(train_pred) / len(train_pred)\n","    return avg * 0.65 + test_pred * 0.35"]},{"cell_type":"code","execution_count":68,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1655389288647,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"ppRuz1MVrteC","outputId":"b0b0d1c1-1139-4cf3-e553-0eceb6ebfc6c"},"outputs":[{"name":"stdout","output_type":"stream","text":["One ./food11-hw13/evaluation sample ./food11-hw13/evaluation/0000.jpg\n"]}],"source":["# create dataloader for evaluation\n","eval_set = FoodDataset(os.path.join(cfg['dataset_root'], \"evaluation\"), mode='test')\n","eval_loader = DataLoader(eval_set, batch_size=cfg['batch_size'], shuffle=False, num_workers=0, pin_memory=True)"]},{"cell_type":"code","execution_count":69,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":10,"status":"ok","timestamp":1655389288648,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"PxQJyn6hJ9If","outputId":"787a2b70-0114-4c45-ee38-8383f2151264"},"outputs":[{"data":{"text/plain":["MobileNetV2(\n","  (features): Sequential(\n","    (0): Sequential(\n","      (0): Conv2d(3, 32, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n","      (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","    (1): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=32, bias=False)\n","        (1): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU6(inplace=True)\n","        (3): Conv2d(32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (4): BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (2): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2d(16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU6(inplace=True)\n","        (3): Conv2d(96, 96, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=96, bias=False)\n","        (4): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU6(inplace=True)\n","        (6): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(24, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (3): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2d(24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU6(inplace=True)\n","        (3): Conv2d(144, 144, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=144, bias=False)\n","        (4): BatchNorm2d(144, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU6(inplace=True)\n","        (6): Conv2d(144, 32, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (4): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2d(32, 192, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU6(inplace=True)\n","        (3): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=192, bias=False)\n","        (4): BatchNorm2d(192, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU6(inplace=True)\n","        (6): Conv2d(192, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (5): InvertedResidual(\n","      (conv): Sequential(\n","        (0): Conv2d(64, 384, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (1): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (2): ReLU6(inplace=True)\n","        (3): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n","        (4): BatchNorm2d(384, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","        (5): ReLU6(inplace=True)\n","        (6): Conv2d(384, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","        (7): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      )\n","    )\n","    (6): Sequential(\n","      (0): Conv2d(64, 100, kernel_size=(1, 1), stride=(1, 1), bias=False)\n","      (1): BatchNorm2d(100, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n","      (2): ReLU6(inplace=True)\n","    )\n","  )\n","  (classifier): Linear(in_features=100, out_features=11, bias=True)\n",")"]},"execution_count":69,"metadata":{},"output_type":"execute_result"}],"source":["results = []\n","preds = []\n","student_model_best = get_student_model()\n","ckpt_path = f\"{save_path}/student_best.ckpt\"\n","student_model_best.load_state_dict(torch.load(ckpt_path, map_location='cpu'))\n","student_model_best.to(device)"]},{"cell_type":"code","execution_count":70,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":49,"referenced_widgets":["1b9c2d75d839483e8eabd89894ee0e50","16c06119e71f41faabd62d21de90db62","c9844083a04147e7b24d98679a05a3e1","18bfd72f2cf148429e10a77b36310b32","e2b6f3a1132f427ab399653af7f9a07d","5a516d6bd48e44b291941bcc243a65f7","0d439e735cf4407c823fd9a7ee9ced36","fd6bc031474949e0ae7f5f7087ac511d","8bdfaff0d8374a2794bc5b0f144ed041","bd0d1a69782b4754a8d4c36924075b40","3ba0d6b1ac0842108d17e5977dcaa8c0"]},"executionInfo":{"elapsed":87101,"status":"ok","timestamp":1655389375743,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"PZf-irsBKIj1","outputId":"508ed631-b843-4147-cc1f-55b4eb6553f4"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 53/53 [00:32<00:00,  1.61it/s]\n"]}],"source":["student_model_best.eval()\n","with torch.no_grad():\n","    for train_im, test_im, _ in tqdm(eval_loader):\n","        test_pred = TTA(student_model_best, train_im, test_im)\n","        preds.append(test_pred)\n","results.append(torch.cat(preds))"]},{"cell_type":"code","execution_count":71,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1655389375745,"user":{"displayName":"李芸芳","userId":"04520645323018691385"},"user_tz":-480},"id":"nhrlbzOpKKsr"},"outputs":[],"source":["results = torch.cat(results, dim=0)\n","\n","test_label = np.argmax(results.cpu().data.numpy(), axis=1)\n","prediction = test_label.squeeze().tolist()\n","\n","\n","def pad4(i):\n","    return \"0\"*(4-len(str(i)))+str(i)\n","\n","\n","df = pd.DataFrame()\n","df[\"Id\"] = [pad4(i) for i in range(0, len(eval_set))]\n","df[\"Category\"] = prediction\n","df.to_csv(f\"{save_path}/submission_kfold.csv\", index=False)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyOiAlJUnxON8AcLCyrCl0w9","background_execution":"on","collapsed_sections":[],"machine_shape":"hm","name":"hw13_with_MobileNetandBetterParam.ipynb","provenance":[{"file_id":"1g4wBLZWWeavKnXoSEq7hSmTUo_t3GAlO","timestamp":1655292611434}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.7"},"widgets":{"application/vnd.jupyter.widget-state+json":{"0d439e735cf4407c823fd9a7ee9ced36":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"16c06119e71f41faabd62d21de90db62":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_5a516d6bd48e44b291941bcc243a65f7","placeholder":"​","style":"IPY_MODEL_0d439e735cf4407c823fd9a7ee9ced36","value":"100%"}},"18bfd72f2cf148429e10a77b36310b32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_bd0d1a69782b4754a8d4c36924075b40","placeholder":"​","style":"IPY_MODEL_3ba0d6b1ac0842108d17e5977dcaa8c0","value":" 27/27 [01:27&lt;00:00,  2.50s/it]"}},"1b9c2d75d839483e8eabd89894ee0e50":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_16c06119e71f41faabd62d21de90db62","IPY_MODEL_c9844083a04147e7b24d98679a05a3e1","IPY_MODEL_18bfd72f2cf148429e10a77b36310b32"],"layout":"IPY_MODEL_e2b6f3a1132f427ab399653af7f9a07d"}},"3ba0d6b1ac0842108d17e5977dcaa8c0":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5a516d6bd48e44b291941bcc243a65f7":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8bdfaff0d8374a2794bc5b0f144ed041":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"bd0d1a69782b4754a8d4c36924075b40":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c9844083a04147e7b24d98679a05a3e1":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_fd6bc031474949e0ae7f5f7087ac511d","max":27,"min":0,"orientation":"horizontal","style":"IPY_MODEL_8bdfaff0d8374a2794bc5b0f144ed041","value":27}},"e2b6f3a1132f427ab399653af7f9a07d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fd6bc031474949e0ae7f5f7087ac511d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
