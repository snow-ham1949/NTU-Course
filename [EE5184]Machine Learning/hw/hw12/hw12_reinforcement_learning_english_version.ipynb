{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fp30SB4bxeQb"
      },
      "source": [
        "# **Homework 12 - Reinforcement Learning**\n",
        "\n",
        "If you have any problem, e-mail us at ntu-ml-2022spring-ta@googlegroups.com\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXsnCWPtWSNk"
      },
      "source": [
        "## Preliminary work\n",
        "\n",
        "First, we need to install all necessary packages.\n",
        "One of them, gym, builded by OpenAI, is a toolkit for developing Reinforcement Learning algorithm. Other packages are for visualization in colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# your workspace in your drive\n",
        "workspace = 'ML2021-hw12'\n",
        "\n",
        "try:\n",
        "   os.chdir(os.path.join('/content/gdrive/MyDrive/', workspace))\n",
        "except:\n",
        "   os.mkdir(os.path.join('/content/gdrive/MyDrive/', workspace))\n",
        "   os.chdir(os.path.join('/content/gdrive/MyDrive/', workspace))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5e2bScpnkVbv",
        "outputId": "7820b1d8-6bda-41bf-c9cc-d459a67c0129",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "!apt update\n",
        "!apt install python-opengl xvfb -y\n",
        "!pip install gym[box2d]==0.18.3 pyvirtualdisplay tqdm numpy==1.19.5 torch==1.8.1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M_-i3cdoYsks"
      },
      "source": [
        "\n",
        "Next, set up virtual display，and import all necessaary packages."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nl2nREINDLiw",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "from pyvirtualdisplay import Display\n",
        "virtual_display = Display(visible=0, size=(1400, 900))\n",
        "virtual_display.start()\n",
        "\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.distributions import Categorical\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaEJ8BUCpN9P"
      },
      "source": [
        "# Warning ! Do not revise random seed !!!\n",
        "# Your submission on JudgeBoi will not reproduce your result !!!\n",
        "Make your HW result to be reproducible.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fV9i8i2YkRbO",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "seed = 543 # Do not change this\n",
        "def fix(env, seed):\n",
        "  env.seed(seed)\n",
        "  env.action_space.seed(seed)\n",
        "  torch.manual_seed(seed)\n",
        "  torch.cuda.manual_seed(seed)\n",
        "  torch.cuda.manual_seed_all(seed)\n",
        "  np.random.seed(seed)\n",
        "  random.seed(seed)\n",
        "  torch.set_deterministic(True)\n",
        "  torch.backends.cudnn.benchmark = False\n",
        "  torch.backends.cudnn.deterministic = True"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "He0XDx6bzjgC"
      },
      "source": [
        "Last, call gym and build an [Lunar Lander](https://gym.openai.com/envs/LunarLander-v2/) environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N_4-xJcbBt09",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "import gym\n",
        "import random\n",
        "env = gym.make('LunarLander-v2')\n",
        "fix(env, seed) # fix the environment Do not revise this !!!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NrkVvTrvWZ5H"
      },
      "source": [
        "## What Lunar Lander？\n",
        "\n",
        "“LunarLander-v2”is to simulate the situation when the craft lands on the surface of the moon.\n",
        "\n",
        "This task is to enable the craft to land \"safely\" at the pad between the two yellow flags.\n",
        "> Landing pad is always at coordinates (0,0).\n",
        "> Coordinates are the first two numbers in state vector.\n",
        "\n",
        "![](https://gym.openai.com/assets/docs/aeloop-138c89d44114492fd02822303e6b4b07213010bb14ca5856d2d49d6b62d88e53.svg)\n",
        "\n",
        "\"LunarLander-v2\" actually includes \"Agent\" and \"Environment\". \n",
        "\n",
        "In this homework, we will utilize the function `step()` to control the action of \"Agent\". \n",
        "\n",
        "Then `step()` will return the observation/state and reward given by the \"Environment\"."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bIbp82sljvAt"
      },
      "source": [
        "### Observation / State\n",
        "\n",
        "First, we can take a look at what an Observation / State looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rsXZra3N9R5T",
        "outputId": "82e9e3ab-099d-4c0a-db3b-d6e78bad214b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(env.observation_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezdfoThbAQ49"
      },
      "source": [
        "\n",
        "`Box(8,)`means that observation is an 8-dim vector\n",
        "### Action\n",
        "\n",
        "Actions can be taken by looks like"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p1k4dIrBAaKi",
        "outputId": "bdcbaf22-f826-4f64-9991-64206f374fda",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(env.action_space)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dejXT6PHBrPn"
      },
      "source": [
        "`Discrete(4)` implies that there are four kinds of actions can be taken by agent.\n",
        "- 0 implies the agent will not take any actions\n",
        "- 2 implies the agent will accelerate downward\n",
        "- 1, 3 implies the agent will accelerate left and right\n",
        "\n",
        "Next, we will try to make the agent interact with the environment. \n",
        "Before taking any actions, we recommend to call `reset()` function to reset the environment. Also, this function will return the initial state of the environment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pi4OmrmZgnWA",
        "outputId": "41f0b315-a5d9-47f9-b742-94eb3f67242a",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "initial_state = env.reset()\n",
        "print(initial_state)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBx0mEqqgxJ9"
      },
      "source": [
        "Then, we try to get a random action from the agent's action space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxkOEXRKgizt",
        "outputId": "03b68bf0-a25e-4b94-d686-1b3ca6402338",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "random_action = env.action_space.sample()\n",
        "print(random_action)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mns-bO01g0-J"
      },
      "source": [
        "More, we can utilize `step()` to make agent act according to the randomly-selected `random_action`.\n",
        "The `step()` function will return four values:\n",
        "- observation / state\n",
        "- reward\n",
        "- done (True/ False)\n",
        "- Other information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_WViSxGgIk9",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "observation, reward, done, info = env.step(random_action)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yK7r126kuCNp",
        "outputId": "4c00ab4a-6f08-4d4a-df5f-917ad6d3ca52",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(done)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKdS8vOihxhc"
      },
      "source": [
        "### Reward\n",
        "\n",
        "\n",
        "> Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vxQNs77hi0_7",
        "outputId": "2fea6e0d-1b91-4871-8251-a62cc8b7532e",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "print(reward)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhqp6D-XgHpe"
      },
      "source": [
        "### Random Agent\n",
        "In the end, before we start training, we can see whether a random agent can successfully land the moon or not."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 640
        },
        "id": "Y3G0bxoccelv",
        "outputId": "8d80bae2-d1e1-4c12-bef2-8f8fcaf53c12",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "env.reset()\n",
        "\n",
        "img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "done = False\n",
        "while not done:\n",
        "    action = env.action_space.sample()\n",
        "    observation, reward, done, _ = env.step(action)\n",
        "\n",
        "    img.set_data(env.render(mode='rgb_array'))\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5paWqo7tWL2"
      },
      "source": [
        "## Policy Gradient\n",
        "Now, we can build a simple policy network. The network will return one of action in the action space."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J8tdmeD-tZew",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "#reference: https://github.com/pai4451/ML2021/blob/main/hw12/hw12_dqn.ipynb\n",
        "\n",
        "class DQN(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(DQN, self).__init__()\n",
        "        self.fc1 = nn.Linear(8, 64)\n",
        "        self.fc2 = nn.Linear(64, 64)\n",
        "        self.fc3 = nn.Linear(64, 4)\n",
        "\n",
        "    def forward(self, state):\n",
        "        # map state to action\n",
        "        x = F.relu(self.fc1(state))\n",
        "        x = F.relu(self.fc2(x))\n",
        "        return self.fc3(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynbqJrhIFTC3"
      },
      "source": [
        "Then, we need to build a simple agent. The agent will acts according to the output of the policy network above. There are a few things can be done by agent:\n",
        "- `learn()`：update the policy network from log probabilities and rewards.\n",
        "- `sample()`：After receiving observation from the environment, utilize policy network to tell which action to take. The return values of this function includes action and log probabilities. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zZo-IxJx286z",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "\n",
        "class ReplayMemory:\n",
        "    def __init__(self, CAPACITY):\n",
        "        self.capacity = CAPACITY  \n",
        "        self.memory = []  \n",
        "        self.index = 0  \n",
        "        self.transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "        \n",
        "    def push(self, state, action, state_next, reward):\n",
        "        if len(self.memory) < self.capacity: \n",
        "            self.memory.append(None)\n",
        "\n",
        "        self.memory[self.index] = self.transition(state, action, state_next, reward)\n",
        "\n",
        "        self.index = (self.index + 1) % self.capacity\n",
        "\n",
        "    def sample(self, batch_size):\n",
        "        # random sample\n",
        "        return random.sample(self.memory, batch_size)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.memory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "class DQNAgent():\n",
        "    def __init__(self, num_states, num_actions):\n",
        "        \n",
        "        self.num_states = num_states\n",
        "        self.num_actions = num_actions\n",
        "        self.memory_capacity = 10000\n",
        "        self.memory = ReplayMemory(self.memory_capacity)\n",
        "        self.main_q_network = DQN() \n",
        "        self.target_q_network = DQN()\n",
        "        self.optimizer = optim.RMSprop(self.main_q_network.parameters(), lr=1e-4)\n",
        "    \n",
        "    def update_q_function(self):\n",
        "\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        \n",
        "        self.batch, self.state_batch, self.action_batch, self.reward_batch, self.non_final_next_states = self.make_minibatch()\n",
        "        self.expected_state_action_values = self.get_expected_state_action_values()\n",
        "        self.update_main_q_network()\n",
        "\n",
        "    def make_minibatch(self):\n",
        "        transitions = self.memory.sample(BATCH_SIZE)\n",
        "        Transition = namedtuple('Transition', ('state', 'action', 'next_state', 'reward'))\n",
        "        batch = Transition(*zip(*transitions))\n",
        "        state_batch = torch.cat(batch.state)\n",
        "        action_batch = torch.cat(batch.action)\n",
        "        reward_batch = torch.cat(batch.reward)\n",
        "        non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "        return batch, state_batch, action_batch, reward_batch, non_final_next_states\n",
        "\n",
        "    def get_expected_state_action_values(self):\n",
        "        # calculate Q(S, A)\n",
        "\n",
        "        self.main_q_network.eval()\n",
        "        self.target_q_network.eval()\n",
        "        self.state_action_values = self.main_q_network(self.state_batch).gather(1, self.action_batch)\n",
        "        non_final_mask = torch.BoolTensor(tuple(map(lambda s: s is not None, self.batch.next_state)))\n",
        "        next_state_values = torch.zeros(BATCH_SIZE)\n",
        "        next_state_values[non_final_mask] = self.target_q_network(self.non_final_next_states).max(1)[0].detach()\n",
        "        expected_state_action_values = self.reward_batch + GAMMA * next_state_values\n",
        "        \n",
        "        return expected_state_action_values \n",
        "        \n",
        "    def get_action(self, state, episode, test=False):\n",
        "        if test:\n",
        "            self.main_q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                action = self.main_q_network(torch.from_numpy(state).unsqueeze(0)).max(1)[1].view(1, 1)\n",
        "            return action.item()\n",
        "        \n",
        "        global steps_done\n",
        "        # Epsilon-greedy policy\n",
        "        epsilon = EPS_END + (EPS_START - EPS_END) * \\\n",
        "                np.exp(-1. * steps_done / EPS_DECAY)\n",
        "        \n",
        "        steps_done += 1\n",
        "        \n",
        "        if epsilon <= np.random.uniform(0, 1):\n",
        "            self.main_q_network.eval()\n",
        "            with torch.no_grad():\n",
        "                action = self.main_q_network(state).max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            action = torch.LongTensor([[random.randrange(self.num_actions)]])  \n",
        "            \n",
        "        return action\n",
        "\n",
        "    def update_main_q_network(self):\n",
        "        self.main_q_network.train()\n",
        "        loss = F.smooth_l1_loss(self.state_action_values, self.expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward() \n",
        "        for param in self.main_q_network.parameters():\n",
        "            param.grad.data.clamp_(-1, 1)\n",
        "        self.optimizer.step()\n",
        "\n",
        "\n",
        "    def memorize(self, state, action, state_next, reward):\n",
        "        self.memory.push(state, action, state_next, reward)\n",
        "\n",
        "    def update_target_q_function(self):\n",
        "        self.target_q_network.load_state_dict(self.main_q_network.state_dict())  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ehPlnTKyRZf9"
      },
      "source": [
        "Lastly, build a network and agent to start training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GfJIvML-RYjL",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "network = DQN()\n",
        "agent = DQNAgent(env.observation_space.shape[0], env.action_space.n)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ouv23glgf5Qt"
      },
      "source": [
        "## Training Agent\n",
        "\n",
        "Now let's start to train our agent.\n",
        "Through taking all the interactions between agent and environment as training data, the policy network can learn from all these attempts,"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "30f1bc6f241e4729880fff0775f6bf7f",
            "154182745b5d45389a58f69f01214abf",
            "8d3251f885d047349f8628f7a2c9ed1a",
            "06dfabe998944232abb5489ce6045d2a",
            "99d70e7bee484942959132b8d8f2e261",
            "e740b0be22714c7ea1a8130b9d5ff5f7",
            "ca300afee8834b69833adb0c1cf03937",
            "dbb666fbdffe41fd8b2a8485ad3177f0"
          ]
        },
        "id": "vg5rxBBaf38_",
        "outputId": "e09ff5aa-069c-4e8a-bfa9-73e5a955718b",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "# Switch network into training mode\n",
        "agent.main_q_network.train()\n",
        "agent.target_q_network.train()  \n",
        "EPISODE_PER_BATCH = 5   # update the  agent every 5 episode\n",
        "NUM_BATCH = 600        # totally update the agent for 400 time\n",
        "GAMMA = 0.99\n",
        "EPS_START = 0.9\n",
        "EPS_END = 0.05\n",
        "EPS_DECAY = 200\n",
        "best_score = 0\n",
        "best_batch = 0\n",
        "steps_done = 0\n",
        "\n",
        "avg_total_rewards, avg_final_rewards = [], []\n",
        "\n",
        "prg_bar = tqdm(range(NUM_BATCH))\n",
        "for batch in prg_bar:\n",
        "\n",
        "    rewards = []\n",
        "    total_rewards, final_rewards = [], []\n",
        "\n",
        "    # collect trajectory\n",
        "    for episode in range(EPISODE_PER_BATCH):\n",
        "        \n",
        "        state = env.reset()\n",
        "        state = torch.unsqueeze(torch.from_numpy(state).type(torch.FloatTensor), 0)\n",
        "        total_reward, total_step = 0, 0\n",
        "\n",
        "        while True:\n",
        "\n",
        "            action = agent.get_action(state, batch)\n",
        "            state_next, reward, done, _ = env.step(action.item())\n",
        "\n",
        "            log_probs.append(log_prob) # [log(a1|s1), log(a2|s2), ...., log(at|st)]\n",
        "            # seq_rewards.append(reward)\n",
        "            state = state_next\n",
        "            total_reward += reward\n",
        "            total_step += 1\n",
        "            rewards.append(reward) # change here\n",
        "            # ! IMPORTANT !\n",
        "            # Current reward implementation: immediate reward,  given action_list : a1, a2, a3 ......\n",
        "            #                                                         rewards :     r1, r2 ,r3 ......\n",
        "            # medium：change \"rewards\" to accumulative decaying reward, given action_list : a1,                           a2,                           a3, ......\n",
        "            #                                                           rewards :           r1+0.99*r2+0.99^2*r3+......, r2+0.99*r3+0.99^2*r4+...... ,  r3+0.99*r4+0.99^2*r5+ ......\n",
        "            # boss : implement Actor-Critic\n",
        "            if done:\n",
        "                final_rewards.append(reward)\n",
        "                total_rewards.append(total_reward)\n",
        "                state_next = None\n",
        "                break\n",
        "            else:\n",
        "                state_next = torch.unsqueeze(torch.from_numpy(state_next).type(torch.FloatTensor), 0)\n",
        "            \n",
        "            agent.memorize(state, action, state_next, torch.FloatTensor([reward]))\n",
        "            agent.update_q_function()\n",
        "            state = state_next\n",
        "\n",
        "    print(f\"rewards looks like \", np.shape(rewards))  \n",
        "    print(f\"log_probs looks like \", np.shape(log_probs))     \n",
        "    # record training process\n",
        "    avg_total_reward = sum(total_rewards) / len(total_rewards)\n",
        "    avg_final_reward = sum(final_rewards) / len(final_rewards)\n",
        "    avg_total_rewards.append(avg_total_reward)\n",
        "    avg_final_rewards.append(avg_final_reward)\n",
        "    prg_bar.set_description(f\"Total: {avg_total_reward: 4.1f}, Final: {avg_final_reward: 4.1f}\")\n",
        "    agent.update_target_q_function()\n",
        "    # update agent\n",
        "    # rewards = np.concatenate(rewards, axis=0)\n",
        "    print(\"logs prob looks like \", torch.stack(log_probs).size())\n",
        "    print(\"torch.from_numpy(rewards) looks like \", torch.from_numpy(rewards).size())\n",
        "\n",
        "    fix(env, seed)\n",
        "    agent.main_q_network.eval()    # set the network into evaluation mode\n",
        "    NUM_OF_TEST = 5 # Do not revise this !!!\n",
        "    test_total_reward = []\n",
        "    action_list = []\n",
        "    for i in range(NUM_OF_TEST):\n",
        "        actions = []\n",
        "        state = env.reset()\n",
        "\n",
        "    #   img = plt.imshow(env.render(mode='rgb_array'))\n",
        "\n",
        "        total_reward = 0\n",
        "\n",
        "        done = False\n",
        "        while not done:\n",
        "            action = agent.get_action(state, episode=i, test=True)\n",
        "            actions.append(action)\n",
        "            state, reward, done, _ = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "\n",
        "            #   img.set_data(env.render(mode='rgb_array'))\n",
        "            #   display.display(plt.gcf())\n",
        "            #   display.clear_output(wait=True)\n",
        "        \n",
        "        print(total_reward)\n",
        "        test_total_reward.append(total_reward)\n",
        "\n",
        "        action_list.append(actions) # save the result of testing \n",
        "    if np.mean(test_total_reward) > best_score:\n",
        "        best_score = np.mean(test_total_reward)\n",
        "        best_batch = batch\n",
        "        PATH = \"Action_List_test_best.npy\"\n",
        "        np.save(PATH ,np.array(action_list)) \n",
        "        print('Improve to score %.2f at batch %d'% (best_score, best_batch ))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "c-CqyhHzaWAL",
        "outputId": "f6c36bed-1ece-43a2-e1de-9844a1794323",
        "vscode": {
          "languageId": "python"
        }
      },
      "outputs": [],
      "source": [
        "from google.colab import files\n",
        "files.download(PATH)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "hw12_reinforcement_learning_english_version.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "06dfabe998944232abb5489ce6045d2a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dbb666fbdffe41fd8b2a8485ad3177f0",
            "placeholder": "​",
            "style": "IPY_MODEL_ca300afee8834b69833adb0c1cf03937",
            "value": " 400/400 [11:19&lt;00:00,  1.70s/it]"
          }
        },
        "154182745b5d45389a58f69f01214abf": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "30f1bc6f241e4729880fff0775f6bf7f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8d3251f885d047349f8628f7a2c9ed1a",
              "IPY_MODEL_06dfabe998944232abb5489ce6045d2a"
            ],
            "layout": "IPY_MODEL_154182745b5d45389a58f69f01214abf"
          }
        },
        "8d3251f885d047349f8628f7a2c9ed1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "Total:  86.3, Final:  0.0: 100%",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e740b0be22714c7ea1a8130b9d5ff5f7",
            "max": 400,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_99d70e7bee484942959132b8d8f2e261",
            "value": 400
          }
        },
        "99d70e7bee484942959132b8d8f2e261": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": "initial"
          }
        },
        "ca300afee8834b69833adb0c1cf03937": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dbb666fbdffe41fd8b2a8485ad3177f0": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e740b0be22714c7ea1a8130b9d5ff5f7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
